#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass report
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 4cm
\topmargin 3cm
\rightmargin 4cm
\bottommargin 4cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Report
\end_layout

\begin_layout Author
Luke Tomlin
\end_layout

\begin_layout Date
04/03/13
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Abstract
This project details the development and results of a system that autonomously
 flies a small remote-control helicopter.
 The only measurement data is obtained from a standard Kinect Camera, and
 this data is used to make assumptions on the flight properties of the helicopte
r, and then fly the helicopter accordingly.
 These assumptions are re-evaluated as the helicopter is flown.
 
\end_layout

\begin_layout Subsection*
Acknowledgements
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
The projects main objectives can be summarised by one sentence - to mimic
 how a person would learn to fly and operate a small remote control helicopter
 in an indoor environment, using non-specialized hardware.
 The problem is best described by the limiting factors - there is a room
 indoors, a small remote-controlled helicopter, a computer and a simple
 camera source capable of estimating depth to some degree of accuracy (also
 known as an RGB-D camera).
 With these boundaries, the final aim is to have the computer learn to fly
 the helicopter (with some pre-instilled understanding of the mechanics
 of helicopter flying), and given goals from a human, enact them correctly.
 The difficulties are in the details - how exactly does one learn to fly
 a helicopter? How can positional information be extracted from the helicopter
 in the room? What is the best way to act out intentions? Each of these
 question poses an interesting engineering challenge, that can be in most
 cases be tackled in an entirely modular fashion.
 In addition to the challenge itself, the final product of this project
 should be more than the sum of its parts - it should be fun to use, to
 watch, and to play with!
\begin_inset Newline newline
\end_inset

The simple RGB-D camera that will be used in this project is Kinect.
 It is capable of generating real-time depth maps, which represent the depth
 of the points visible to the camera.
 This data can be viewed as a greyscale image (scaling the depth values
 from 0 to 255) or can be extrapolated into the 3D environment using the
 information about camera.
 These readings are not always accurate, and may fluctuate in a seemingly
 motionless scene.
 In addition to the depth camera on board the Kinect, it has a simple RGB
 camera capable of recording in 640x480.
\begin_inset Newline newline
\end_inset

In addition to the connect, all other devices and software are generic and
 easily obtainable, all costing under Â£30 (excluding the computer).
\end_layout

\begin_layout Section
Deconstruction
\end_layout

\begin_layout Standard
The first step is to try and separate the individual programming problems
 into a way that is easily reasoned about.
 Once they have been divided, research can be done on the specific problems,
 and tools can be created to start tackling the problems at hand.
 For instance, the computer vision problem is entirely separate from that
 of the machine learning - the machine operates under the assumption that
 the positional information is mostly accurate, and it is up to the computer
 vision module to provide these mostly accurate readings.
\end_layout

\begin_layout Paragraph
Mimicry
\end_layout

\begin_layout Standard
I decided to tackle this problem by thinking about how a human would go
 about it, reducing it to the key problems and then thinking about how a
 machine could go about tackling them.
 The final project will be split into modules, each providing a different
 role.
\end_layout

\begin_layout Paragraph
Computer Vision
\end_layout

\begin_layout Standard
There are a multitude of tasks that need to be performed from a vision perspecti
ve before the overall aims can be realised.
 I aim to start from as simplified as possible, simply tracking an object
 on a 2D white background, and then from this gradually iterate the vision
 module, adding extra functionality, accuracy, and dimensions.
 Each iteration will present its own challenges, from getting depth information
 to tracking orientation of the helicopter.
\end_layout

\begin_layout Paragraph
Machine Learning
\end_layout

\begin_layout Standard
The AI that flies the helicopter is wholly separate from the vision, and
 instead is supposed to recreate the learning aspects of the abstract idea
 of a helicopter.
 Its key requirement is to know exactly how to fly the helicopter, given
 information about the current whereabouts, and know how to act upon the
 orders that it is given.
 
\end_layout

\begin_layout Paragraph
Complexity
\end_layout

\begin_layout Standard
The easiest place to see the complexity in this project is by looking at
 the computer vision aspect.
 Initially I will try to deal with the simplest case - that of a purely
 virtualised environment with completely controlled variables - no computer
 vision required.
 Then, for the first step, I will try to work in 1 dimension only, that
 is up and down.
 This restricts the vision to 1 dimension also, tracking the movement of
 a blob up and down.
 Expanding this to 2 dimensions should not be as much of a problem from
 the computer vision perspective, but requires increased complexity of the
 AI itself - it needs to know 
\begin_inset Formula $how$
\end_inset

 a helicopter moves forward and backwards, using the main propellor for
 upwards and forwards thrust.
 Expanding to 3 dimensions provides even more challenges - from navigating
 a 3 dimension environment (obtaining depth information and object awareness)
 to reasoning about the movements of a helicopter in a 3D environment.
 
\end_layout

\begin_layout Section
Technology
\end_layout

\begin_layout Standard
To aid the realisation of the aims, I plan to use some 
\begin_inset Quotes eld
\end_inset

pre-made
\begin_inset Quotes erd
\end_inset

 technology.
 This will allow me to focus on the project as the whole, rather than in
 the minute details.
\end_layout

\begin_layout Subsection
Kinect
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kinect.png
	display false
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
For the computer vision, I will use a Kinect sensor device, created by Microsoft
, which provides me with both an RGB camera and a method for calculating
 depth of the image.
 Additionally, there are several freely available open-source software kits
 for interacting with the Kinect on any platform.
 This should make the computer vision aspect simpler, as I will be able
 to use the depth map that the kinect generates to augment any results I
 get from the single camera source.
 It is also a very popular device for performing computer vision with -
 this means that there will most likely be many other examples of it being
 used for tasks similar to mine, as well as many ways to incorporate it
 with tools that I might use.
\end_layout

\begin_layout Subsection
OpenCV
\end_layout

\begin_layout Standard
OpenCV (Open Source Computer Vision Library) is a library of programming
 functions that will let me perform image analysis from the Kinect.
 It is free to use, and has many different implementations in different
 programming languages.
 I will talk about this in more detail later on.
\end_layout

\begin_layout Subsection
Arduino
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename arduino.png
	display false
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
Arduino is an open-source microcontroller, which lets the user work with
 a simple programming language (C-like) to control how the circuit works.
 It also comes with a Processing-based IDE for programming in, that handles
 a lot of the communications between the board and the computer.
 Additionally, the board itself is reasonably cheap to purchase and highly
 re-usable.
 This fits very well with the theme of the project as a whole, using off-the-she
lf, affordable products.
\end_layout

\begin_layout Subsection
Syma Remote Control Helicopter
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename syma.png
	display false
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
The Syma Remote Control Helicopter is a small indoors-flying helicopter,
 that is controlled by an IR signal sent from a remote control.
 It is modelled to be easy to fly indoors, and to remain stable even in
 in-experienced hands.
 It does this by not having a traditional tail rotor - instead, it has an
 internal gyroscope that keeps it level, and rotation is accomplished by
 speeding up and slowing down the main rotors.
 The tail rotor is instead positioned facing upwards, and provides a tilting
 force to the tail, letting the helicopter fly forwards and backwards.
 This makes the helicopter extremely stable, at the sacrifice of not being
 able to tilt left or right.
 Ultimately, this only serves to make it easier to fly.
 The details on the communication protocols of the helicopter are not freely
 available, but some third party sources have done their own analysis of
 the signals (using oscilloscopes for example), and this should prove to
 be sufficient.
 These are discussed later.
\end_layout

\begin_layout Paragraph
Basic helicopter mechanics
\end_layout

\begin_layout Standard
The helicopter itself consists of three different rotors (two on the main
 rotor, and one on the tail), and a gyroscope.
 The two sets of blades on the main rotor rotate in different directions,
 the top blades rotating clockwise and the lower ones rotation counter-clockwise.
 This has the effect of cancelling out their torque when they move at the
 same speed, keeping the helicopter from rotating in one particular direction.
 The ratio that these blades move to each other can be altered in small
 amounts for fine tuning.
 As discussed above, the rear rotor is positioned in an upright position,
 just like the main rotors.
\end_layout

\begin_layout Itemize
Vertical acceleration is gained by increasing the throttle to the two main
 rotors in equal amounts.
 
\end_layout

\begin_layout Itemize
Yaw rotation is gained by increasing one of the main rotors, and decreasing
 the other an equivalent amount.
 This will give an overall clockwise or counter-clockwise torque without
 affecting the vertical acceleration produced.
 
\end_layout

\begin_layout Itemize
Pitch movement is gained by spinning the tail rotor.
 Reversing the spin of the tail rotor results in opposite pitch.
 The gyroscope on top of the main helicopter rotor prevents the helicopter
 from pitching forwards too much by providing a counter force.
\end_layout

\begin_layout Subsection
Bullet Physics and simulation
\end_layout

\begin_layout Standard
It is an attractive idea to have a simulation environment in which I can
 model the helicopter with very specific restraints so that I can do extensive
 tests on the Monkey AI.
 One potential idea that I am looking into is somehow connecting the Open
 Source physics library Bullet (http://www.bulletphysics.com/) to my project,
 letting me simulate the helicopter in a very realistic fashion before actually
 trying real world implementation.
 This lets me skip the whole vision part and focus instead on only the AI
 mechanics.
 The library itself is built in C++, and lets the user simulate collision
 detection, rigid body dynamics and soft body dynamics, amongst other things.
\end_layout

\begin_layout Chapter
Background
\end_layout

\begin_layout Standard
The whole basis from this project is from a suggestion by Google engineer
 Johnny Chung Lee, from his blog post 
\begin_inset Quotes eld
\end_inset

Computer Controlling a Syma Helicopter
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Syma"

\end_inset

.
 In his post, he talks about possible methods for setting up the control
 of a remote helicopter, as far as controlling it manually from a computer.
 He then conjectures the possibility of using a camera or similar device
 to control the helicopter, and getting it to hover.
 He leaves the detail of this up to the user, however!
\end_layout

\begin_layout Standard
Armed with this inspiration for a starting point, I took to reading some
 detail for how I would set up the foundations of the project.
 This included browsing some other pages on using Arduino
\begin_inset CommandInset citation
LatexCommand cite
key "Arduino,Syma2"

\end_inset

, IR control of the helicopter
\begin_inset CommandInset citation
LatexCommand cite
key "IR"

\end_inset

 and circuit plans
\begin_inset CommandInset citation
LatexCommand cite
key "Syma2"

\end_inset

.
 There was a lot of trial and error involved with the building of the circuit,
 as I had done any electrical engineering for quite some time.
 Initially I built a simple LED circuit to get a feel for how the Arduino
 itself worked, and then once I was confident with that I built the main
 IR circuit.
 This proved to work well enough, however, there were some teething problems
 - the helicopter would jutter and work sporadically, as if it were not
 always receiving signal.
 The problem became better and worse as I changed the timing values for
 the IR signals, so it was evident that it was a problem with the signal
 timing.
 After doing some further reading on precise timing using the Arduino
\begin_inset CommandInset citation
LatexCommand cite
key "Arduino port"

\end_inset

, and some trial and error, I found the value range that seemed to work
 best, or at least well enough for me.
 
\end_layout

\begin_layout Standard
I took a small amount of time to consider the feasibility of Arduino for
 the task at hand - for instance, would it be possible to do the whole project
 on a Raspberry Pi (http://www.raspberrypi.org/)? Or would the Arduino be
 too restrictive for my requirements? I decided that as the main part of
 this project was based upon the restrictions of scenario, and execution,
 not processing power, it would be needlessly limiting to use a Raspberry
 Pi or other similar device instead of a computer.
 Additionally, a computer would have a much wider range of tools available
 to it, making the project simpler to execute.
 The Arduino could be used as a simple intermediate device, that takes the
 commands from the computer and sends them on to the helicopter, which does
 not require much computational power.
\end_layout

\begin_layout Standard
With my initial goals in mind, I started to build the foundation for the
 project using the programming language Haskell.
 I decided to use Haskell because I knew that it has an extensive programming
 library available to it, has bindings to the OpenCV library (which I may
 or may not use), and I had previously done a group project to create an
 AI framework in Haskell, which seemed appropriate given the topic of this
 project.
 This foundation involved making the various interfaces for the different
 parts of the project (modules, classes etc.), and deciding what responsibilities
 would lie where.
 
\end_layout

\begin_layout Section
Prior work
\end_layout

\begin_layout Standard
Whilst researching methods and learning materials for the project, I came
 across examples of similar work that other people have performed, namely
 using quadcopters.
 Quadcopters (also known as quadrocopters) are small flying machines that
 have four individual propellers arranged in a square.
 Navigation is accomplished by powering the propellers at different speeds,
 causing the platform as a whole to tilt.
 Quadcopters are reknown for their stability and simple design.
\end_layout

\begin_layout Subsection
Quadrocopter experiments, Technical University Munich 
\begin_inset CommandInset citation
LatexCommand cite
key "Camera based nav of low cost quadropter"

\end_inset


\end_layout

\begin_layout Standard
Three students at the Technical University Munich wrote a paper titled 
\begin_inset Quotes eld
\end_inset

Camera-Based Navigation of a Low-Cost Quadrocopter
\begin_inset Quotes erd
\end_inset

, detailing 
\begin_inset Quotes eld
\end_inset

a system that enables a low-cost quadrocopter coupled with a ground-based
 laptop to navigate autonomously in previously unknown and GPS-denied environmen
ts
\begin_inset Quotes erd
\end_inset

.
 This project shares many similar points with mine - it is done using a
 low-cost flying device, without customised, high-precision location-data
 sources, uses (amongst other things) a monocular vision device, and works
 indoors in previously unknown environments.
 However, it is also different in many ways, not just in the fact that it
 is using a quadcopter instead of a helicopter - it also has access to other
 on-board camera sources and flight information (it also has a 3-axis gyroscope
 and accelerometer and an ultrasound altimeter).
 Despite these differences, I should still be able to get some useful informatio
n from this project.
\end_layout

\begin_layout Paragraph*
Monocular SLAM
\end_layout

\begin_layout Standard
SLAM (simultaneous localization and mapping) is a computer vision technique
 used to build up a map of an environment containing unknown factors, whilst
 at the same time keeping track of current location.
 Many different types of sensors can be used to solve SLAM-based problems,
 such as laser range scanners, monocular or stereo cameras, or RGB-D sensors
 (RGB camera plus a depth sensor eg.
 Kinect).
 It is used in the quadropter experiments to keep track of the quadropter
 relative to the camera.
\end_layout

\begin_layout Paragraph*
Extended Kalman Filter
\end_layout

\begin_layout Standard
In the experiments, they use an Extended Kalman Filter (EKF) to combine
 all of the available data into a single, reliable data point.
 In addition, the EKF is used to compensate for different time delays that
 occur in the system (for example, from data transmission delays, or from
 computationally expensive calculations).
 The EKF differs from the standard Kalman Filter in the fact that is nonlinear.
 
\end_layout

\begin_layout Paragraph*
Results
\end_layout

\begin_layout Standard
In the quadropter experiments, they successfully demonstrated that accurate
 and robust visual navigation is feasible using a simple, low-cost hardware,
 in a variety of environments.
 Using a combination of machine learning and visual systems from a monocular
 source, combined with onboard data, they managed to achieve an average
 positioning accuracy of 4.9cm whilst flying indoors.
 This is quite encouraging for my project - whilst I will not have access
 to onboard flying information, my location information does not need to
 be this accurate.
 The methods that they describe for using an EKF to measure the quadropter
 position at different observational times subject to control transmission
 delay should prove to be very useful for when I am also doing visual tracking,
 as I will be subject to many of the same problems that they faced.
 When I get to this point in my project, it will be worth doing extra research
 on the methods that they used, and how I can implement it.
\end_layout

\begin_layout Subsection
Autonomous Camera-Based Navigation of a Quadrocopter
\begin_inset CommandInset citation
LatexCommand cite
key "Autonomous Camera based quadropter"

\end_inset


\end_layout

\begin_layout Standard
In his master's thesis, Jacob Engel discusses a system for enabling a quadrocopt
er to 
\begin_inset Quotes eld
\end_inset

localize and navigate autonomously in previously unknown and GPS-denied
 environments...
 using a monocular camera onboard the quadrocopter
\begin_inset Quotes erd
\end_inset

.
 Much like the paper discussed above, he uses a SLAM system for positional
 estimates, and combines this with an Extended Kalman Filter to fuse all
 available data and to compensate for data delays.
 He talks in detail about SLAM and the EKF, which is the key part of this
 paper that interests me - absolute accuracy of movement are not a key aim
 of this project, just as it is not when the human pilot wishes to fly the
 remote control helicopter.
 However, the paper still raises many interesting points regarding how positiona
l information is gathered, and how flight paths are calculated, which will
 come in useful when I reach the midpoint of my project.
\end_layout

\begin_layout Subsection
Odometry from RGB-D Cameras for Autonomous Quadrocopters
\begin_inset CommandInset citation
LatexCommand cite
key "Odometry quadropter"

\end_inset


\end_layout

\begin_layout Standard
Again following on the themes of the previous two works, this paper focuses
 on the details of flying a quadropter - however, unlike the previous two
 pieces this one details quadropters where the camera source is an RGB-D
 camera located on board the quadropter itself.
 This is a more high-cost solution, but may still provide some insights
 towards control of the quadropter and flight itself.
\end_layout

\begin_layout Paragraph
Least Squares
\end_layout

\begin_layout Standard
The paper discusses the technique of least squares, that is used to estimate
 the parameter of a model that uses noisy observations.
 It also discusses the derivation of this technique, as well as the technique
 of weighted least squares, which is used to deal with outlier observations,
 that heavily affect the parameter estimates.
 Outliers will most likely be something that will occur when I take measurements
 in my project, so making the motion estimates robust is extremely important.
 These outliers are particularly bad in a standard least squares solution,
 because of the large side-effects caused by the quadratic error term in
 its formulation.
 
\end_layout

\begin_layout Paragraph
Remarks
\end_layout

\begin_layout Standard
The methods described for least squares, and dealing with outliers by using
 variations thereof should prove to be useful for the learning part of the
 project.
 
\end_layout

\begin_layout Subsection
KinectFusion: Real-time 3D Reconstruction and Interaction Using a Moving
 Depth Camera
\begin_inset CommandInset citation
LatexCommand cite
key "Kinectfusion"

\end_inset


\end_layout

\begin_layout Standard
The fundamental topics of this paper are somewhat different than mine, however
 I felt that it might provide some useful insight into some of the later
 stages of the project.
 The KinectFusion is a method for creating a 3D representation of a scene
 by using a Kinect, that uses depth values from multiple angles to create
 the model, in real-time.
 It might be possible to utilize some of the techniques used to create a
 partial construction of the environment by using a single location from
 the Kinect.
\end_layout

\begin_layout Paragraph
KinectFusion
\end_layout

\begin_layout Standard
The Kinect, whilst being a great tool for collecting depth data as well
 as RGB data, is still subject to small errors in its depth calculations.
 This means that when we want to recreate the environment as a point map
 when using the depth data, we have to be aware that the resulting model
 is not necessarily accurate.
 The interactive reconstruction system that is KinectFusion aims to solve
 these, by combining different viewpoints of the scene into on single 3D
 representation.
 The Kinect is moved around the scene as a handheld scanner, recreating
 the model in realtime.
 
\end_layout

\begin_layout Paragraph
GPU Implementation
\end_layout

\begin_layout Standard
The real-time camera tracking and reconstruction is performed using parallel
 execution on a GPU.
 If I wanted to use this technology directly, I may need to get specialised
 hardware.
 Alternatively, I may be able to implement a less-functional version using
 my current hardware.
\end_layout

\begin_layout Paragraph
Remarks
\end_layout

\begin_layout Standard
The KinectFusion offers many interesting thinking points.
 Partial scene reconstruction would certainly be useful for the later stages
 of this project, where it is important to be aware of the surroundings
 relative to the location of the helicopter.
 Additionally, it may be of interest to see if this might aid object tracking
 of the helicopter by building a 3D model of it prior to flight.
 
\end_layout

\begin_layout Subsection
Computer Controlling a Syma Helicopter
\begin_inset CommandInset citation
LatexCommand cite
key "Syma"

\end_inset


\end_layout

\begin_layout Standard
This blog post was the original inspiration for this project, and lists
 a potential implementation method for the manual control of the helicopter
 from a computer.
 Additionally, it lists some rough measurements for the IR protocol, and
 it was from these measurements that I based my initial values on.
\end_layout

\begin_layout Paragraph
IR Protocol
\end_layout

\begin_layout Itemize
The signal is modulated at 38KHz
\end_layout

\begin_layout Itemize
The packet header is 2ms on, the 2ms off.
\end_layout

\begin_layout Itemize
The total packet payload is 4 bytes, in big-endian order.
\end_layout

\begin_layout Standard
Then the individual command values are:
\end_layout

\begin_layout Itemize
Yaw, which takes values 0 - 127, 0 being full reverse yaw and 127 being
 full forward yaw.
 63 is no yaw at all.
\end_layout

\begin_layout Itemize
Pitch, which takes values 0 - 127, 0 being full reverse pitch and 127 being
 full forward pitch.
 63 is no pitch at all.
\end_layout

\begin_layout Itemize
Throttle, which takes values 0 - 127, 0 being no throttle and 127 being
 full throttle.
\end_layout

\begin_layout Itemize
Yaw Correction, which takes values 0 - 127, 63 being no correction.
\end_layout

\begin_layout Standard
The packet is completed with a stop bit ('1').
 The individual bit values are formatted as '1' being 
\begin_inset Formula $320\mu s$
\end_inset

 on, then 
\begin_inset Formula $680\mu s$
\end_inset

 off, and '0' being 
\begin_inset Formula $320\mu s$
\end_inset

 on, then 
\begin_inset Formula $280\mu s$
\end_inset

 off.
 These packets should be sent to the helicopter every 120ms.
 
\end_layout

\begin_layout Paragraph
Personal findings
\end_layout

\begin_layout Standard
I implemented the above protocol myself using the Arduino board, and found
 that the helicopter did not respond as I would have liked.
 The helicopter would operate as ordered for a few packets, and then stop
 for a small amount of time, and then restart again.
 This might have been due to small differences in the protocol between my
 helicopter or his, or simply just a problem with his code.
 I figured that this was a problem with the individual timings of packets,
 so did some experiments and found that the following values worked best
 for me:
\end_layout

\begin_layout Itemize
Same values for '1' and '0' pulses, along with message length and the header.
\end_layout

\begin_layout Itemize
Footer of length 
\begin_inset Formula $300\mu s$
\end_inset

 .
\end_layout

\begin_layout Itemize
A delay of 
\begin_inset Formula $25\mu s$
\end_inset

 between each message sent.
\end_layout

\begin_layout Itemize
Compensating time for the individual port switches (basically compensating
 for communication time).
\end_layout

\begin_layout Standard
This resulted in stable, non-stuttering flight that worked from a large
 distance.
 
\end_layout

\begin_layout Section
Control models
\end_layout

\begin_layout Standard
A key consideration I had to make was what method I would use to 'learn'
 the correspondence between resultant acceleration and input power to the
 helicopter.
 The mapping between these two would be extremely important in flying the
 helicopter later, as I would want the helicopter to be able to make judgements
 on the appropriate input for a desired acceleration, whilst also being
 able to augment this mapping with new observations.
 For the straight up/down case, where changing the input throttle results
 in a flat increase in propellor speed, and hence change in upwards velocity
 of the helicopter, I pulled out the key requirements:
\end_layout

\begin_layout Itemize
It needs to be able to return a mapping between acceleration and power.
 
\end_layout

\begin_layout Itemize
It needs to be able to update itself when receiving a new input power and
 the observed output acceleration.
\end_layout

\begin_layout Itemize
It needs to be able to create an initial model with very little input at
 all, that can rapidly re-evaluate itself using the above two methods.
\end_layout

\begin_layout Standard
Some initial considerations were using least squares and various similar
 deriviations, temporal difference learning, or a kalman filter of sorts.
 At first I implemented a basic least squares algorithm, as it appeared
 to be the simplest of the three and would be easy to implement.
 Additionally, by implementing it I might be able to see any problems that
 could emerge if I were to do any other methods in the future.
\end_layout

\begin_layout Subsection
Least Squares 
\end_layout

\begin_layout Standard
I performed some initial research on least squares algorithms, and how appropria
te they would be to the problem at hand.
 Initially, I was more interested in looking for an algorithm that would
 be iterative, and not require recomputation on the entire data set every
 time a new piece of data arrived.
 This was because I was afraid that the computations would quickly become
 slow as the array of measurements became larger and larger.
 Some sources discussed recursive versions of least squares estimation 
\begin_inset CommandInset citation
LatexCommand cite
key "Least squares + Recursive,Least squares wiki"

\end_inset

, but after spending some time looking at the algorithms and realising my
 own lack of knowledge on the subject of regression analysis, I decided
 to first implement the simple least squares algorithm, and then possibly
 look at the recursive version later.
 Without too many problems I managed to implement the basic least squares
 algorithm into my program using a couple of different sources for inspiration
\begin_inset CommandInset citation
LatexCommand cite
key "Least squares wiki"

\end_inset

.
 To ensure that the calculations were as efficient as they could be, I made
 use of the Numeric.LinearAlgebra haskell package (http://hackage.haskell.org/packa
ges/archive/hmatrix/0.8.3.1/doc/html/Numeric-LinearAlgebra-Algorithms.html)
 which provides a bunch of data structures and functions for performing
 linear algebra.
 Doing some initial tests in ghci, this proved to be quite efficient, with
 satisfactory solve times for sample sets of sizes up to 1000, which, considerin
g a seemingly suitable sample time of 100ms, results in a respectable 100
 seconds of input data to base flying judgements on.
 
\end_layout

\begin_layout Standard
In addition to the normal least squares implementation, I was able to do
 some tests with a weighted least squares variant
\begin_inset CommandInset citation
LatexCommand cite
key "Least squares wiki"

\end_inset

.
 This involves giving each input/output measurement a weight, and the resulting
 map is skewed based on this weight.
 The benefit of this would be potentially letting me add 'ages' to the inputs,
 giving more recent and presumably thus more accurate depictions of the
 current state of the helicopter higher priority.
 
\end_layout

\begin_layout Standard
One of the other advantages of the least squares implementation was that
 it would let me easily change the model from linear to quadratic, depending
 on what the underlying system might represent.
 
\end_layout

\begin_layout Paragraph
Potential problems
\end_layout

\begin_layout Standard
In the experiments I encountered some flaws in the usage of least squares
 - unfortunately, due to the nature of how least squares is formulated,
 outliers and highly erroneous results heavily affect the final approximation.
 This meant that when I would get an outlying result (possibly from bad
 timing measurements on the computer, or from blips in the sensor data),
 the whole approximation would be heavily skewed because of this.
 I partially solved this by employing a sanity check on the results as they
 came in, doing some analysis on already received 'okay' results and comparing
 the what I know should happen with the result.
 For instance, I know that the function between power and acceleration of
 the helicopter should be monotonically increasing.
\end_layout

\begin_layout Paragraph
Conclusions
\end_layout

\begin_layout Standard
There are many different forms of least squares fitting - I will have to
 do extra work and experimentation to find the one that is right for what
 I need.
 
\end_layout

\begin_layout Subsection
Kalman Filters
\end_layout

\begin_layout Standard
Kalman Filters were mentioned in many of the previous papers that I read,
 and were used to combine different (potentially noisy) data sources into
 a single result.
 This will most likely apply to my project as well, particularly in the
 vision aspect.
 Raw vision data from the camera will be noisy, and prone to small shifts
 in direction that may not be accurate.
\end_layout

\begin_layout Paragraph
Definition
\end_layout

\begin_layout Standard
The Kalman filter is designed to address the problem of dealing with measurement
s that contains small inaccuracies, producing results that are often more
 accurate than simply using the raw measurements themselves.
 Additionally, it operates recurisvely and as such is efficient to use in
 space and time.
 It is described as working in a two-step process
\begin_inset CommandInset citation
LatexCommand cite
key "Kalman wiki"

\end_inset

 - first the filter produces an estimate of the currest state, along with
 uncertainty values.
 Then, given a measurement of the state (which may contain errors), the
 estimate is updated based upon the certainty.
\end_layout

\begin_layout Standard
More formally, the true state is described as (for some state 
\begin_inset Formula $x$
\end_inset

)
\begin_inset Formula 
\[
x_{k}=Ax_{k-1}+Bu_{k-1}+w_{k-1}
\]

\end_inset

where 
\begin_inset Formula $x_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 true state, 
\begin_inset Formula $A$
\end_inset

 is the state transition model from 
\begin_inset Formula $x_{i-1}$
\end_inset

, 
\begin_inset Formula $B$
\end_inset

 is the control-input model applied to control input 
\begin_inset Formula $u_{i}$
\end_inset

 , and 
\begin_inset Formula $w_{k-1}$
\end_inset

 is the process noise.
 The 
\begin_inset Formula $k^{th}$
\end_inset

observation is made of the true state 
\begin_inset Formula $x_{k}$
\end_inset

,
\begin_inset Formula 
\[
z_{k}=Hx_{k}+v_{k}
\]

\end_inset

where 
\begin_inset Formula $H$
\end_inset

 is an observation model mapping true state to an observation, and 
\begin_inset Formula $v_{k}$
\end_inset

is the observation noise.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Kalman wiki,kalman tutorial"

\end_inset

 Assumptions are made about the properties of the errors.
\end_layout

\begin_layout Paragraph
Suitability
\end_layout

\begin_layout Standard
As the Kalman filter is a recursive estimator (it only requires information
 from the previous time step, and the current state measurement, to get
 the next estimate), it can be used easily in real-time, which means that
 it should fit well into the vision module of the project.
 At the very least, it is worth trying to implement on a basic level to
 see if it improves the quality of the vision tracking.
\end_layout

\begin_layout Subsection
PID Controller
\end_layout

\begin_layout Standard
The majority of the papers mentioned above used a PID Controller (Proportional
 Integral Derivative Controller) as a control mechanism.
 The PID controller is a simple and robust method of controlling a device
 by performing control loop feedback.
\end_layout

\begin_layout Paragraph
Formulation
\end_layout

\begin_layout Standard
The PID mechanism is best described by its three constituent parts; a proportion
al term, integral term, and a derivative term.
 The sum of these three terms makes the new input parameter.
 That is,
\begin_inset Formula 
\[
input=proportional+integral+derivative
\]

\end_inset

Each of the three terms is comprised of an input parameter (
\begin_inset Formula $K_{p},K_{i}$
\end_inset

and 
\begin_inset Formula $K_{d}$
\end_inset

 for proportional, integral and derivative respectively), and some combination
 of the previous and current error in the sytem (the error being the difference
 between the current state and the desired state).
 These are described in more detail below.
\end_layout

\begin_layout Paragraph
Proportional term
\end_layout

\begin_layout Standard
The proportional term is defined as 
\begin_inset Formula 
\[
K_{p}e(t)
\]

\end_inset

where 
\begin_inset Formula $e(t)$
\end_inset

 is the error at time 
\begin_inset Formula $t$
\end_inset

.
 As described by its name, this term increases proportional to the size
 of the error between the desired and current state.
 As such, large 
\begin_inset Formula $K_{p}$
\end_inset

 values will cause a large change in the input value as the error change,
 which can cause system instability (see later for more on stability).
\end_layout

\begin_layout Paragraph
Integral term
\end_layout

\begin_layout Standard
The integral term is defined as
\begin_inset Formula 
\[
K_{i}\int_{0}^{t}e(x)dx
\]

\end_inset

where 
\begin_inset Formula $\int_{0}^{t}e(x)dx$
\end_inset

 is the cumulative error over time.
 This essentially accounts for error that should have previously been corrected
 by the system, and grows as this error perpetuates.
\end_layout

\begin_layout Paragraph
Derivative term
\end_layout

\begin_layout Standard
The derivative term is defined as
\begin_inset Formula 
\[
K_{d}\frac{d}{dt}e(t)
\]

\end_inset

or in a discrete system, simply
\begin_inset Formula 
\[
K_{d}(e(t)-e(t-1))
\]

\end_inset

which is the rate of change of the error.
 This accounts for the 
\begin_inset Quotes eld
\end_inset

approach speed
\begin_inset Quotes erd
\end_inset

 of the device, predicting the 
\begin_inset Quotes eld
\end_inset

velocity
\begin_inset Quotes erd
\end_inset

 of the error and dampening it accordingly.
\end_layout

\begin_layout Paragraph
Tuning
\end_layout

\begin_layout Standard
Each of the parameters reflects a different property of the control loop
 that needs to be addressed.
 The proportional parameter reflects the response to error - small values
 will not have a significant effect on the input, however large errors can
 cause unwanted oscillatory behaviour.
 The integral parameter deal with accumulated error - it is in this way
 that it can compensate for 'droop' in the system, where there is a control
 bias that needs to be accounted for.
 Small values will take a long time to compensate for these, but large values
 will overcompensate causing overshooting.
 The derivative term 'dampens' the approach, but will increase the time
 required to reach stability.
 
\begin_inset Newline newline
\end_inset

Each of the costs and benefits of these terms needs to be weighted against
 the other, and this process is referred to as 'tuning' the controller.
 There are a variety of methods for tuning the controller, from simple manual
 observation and adjustment to full automated methods.
\end_layout

\begin_layout Paragraph
Suitability
\end_layout

\begin_layout Standard
The PID controller was used in the majority of the quadcopter papers, so
 it definitely has a use in automated flying.
 For the helicopter in particular, it fits the requirements very well -
 in the vertical case, for instance, the input is the throttle and the output
 is the observed position, with the error being the difference between the
 required position and the current position.
\end_layout

\begin_layout Section
Vision
\end_layout

\begin_layout Standard
As has been mentioned earlier, I have decided to use a Kinect camera as
 my vision input source.
 The Kinect is not just an RGB camera - it is also a fairly accurate depth
 sensor, and this extra dimension of input could potentially make many of
 the problems that I am trying to solve easier.
 Upon receiving the Kinect from my supervisor, I set about getting it up
 and running with the Ubuntu operating system that I work on, searching
 for libraries and tools for it.
 In addition to this, it was important that I would be able to get the Kinect
 working in Haskell without too much problem, otherwise I would have to
 write a tool for this as well.
 Additionally, there are several packages that support the Kinect for use
 in Haskell.
 However, most of these are incomplete, so I might not use them for my final
 implementation of the vision package.
\end_layout

\begin_layout Subsection
OpenCV
\end_layout

\begin_layout Standard
For my initial research on OpenCV, I began reading some tutorials and online
 documentation
\begin_inset CommandInset citation
LatexCommand cite
key "OpenCV book"

\end_inset

, as well as looking at existing examples in the Haskell wrapper for OpenCV
\begin_inset CommandInset citation
LatexCommand cite
key "OpenCV examples"

\end_inset

.
 I aimed to follow the general learning flow from the book (implemented
 in C++), but to do it myself in Haskell, thus hopefully learning both OpenCV
 and the wrapper itself.
 My first aim after doing some basic exercises was to get basic shape recognitio
n working on a plain white background (eg.
 my wall).
 
\end_layout

\begin_layout Standard
Unfortunately, after spending some time trying to get the Haskell wrappers
 to work, I came to the conclusion that the effort required to get the libraries
 up and running in Haskell was not worth the gain.
 Instead, I decided to use one of the better supported languages for OpenCV
 - Python or C++.
 Both of these have very thorough support for the OpenCV library, letting
 me avoid the problem of language-wrestling entirely.
 Python in particular is incredibly simple, not just in language terms but
 in library support - I discovered the open source framework SimpleCV (http://si
mplecv.org/), which is a large collection of libraries written in Python
 with the aim of providing simplicity.
 This should come in especially useful during the early prototyping phases.
 For instance, getting the Kinect RGB image is as simple as:
\begin_inset listings
lstparams "breaklines=true,language=Python,numbers=left,showstringspaces=false,tabsize=2"
inline false
status open

\begin_layout Plain Layout

import SimpleCV
\end_layout

\begin_layout Plain Layout

cam = SimpleCV.Kinect()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

while True:
\end_layout

\begin_layout Plain Layout

	cam.getImage().show()
\end_layout

\end_inset

Due to the modularity of the vision part of the project, it should be comparitiv
ely simple to add a foreign function call to the Haskell code, and then
 implement the whole vision package in a different language.
 
\end_layout

\begin_layout Standard
To make things even simpler, SimpleCV is also documented very well, with
 all of the documentation available online and inside an interpreter.
 Having access to an interpreter makes experimentation very easy, especially
 considering the fact that Python has first-class functions, letting me
 manipulate images and test out new image processing techniques into existing
 code very easily.
 In addition to the code documentation, I also have started reading the
 book Practical Computer Vision with SimpleCV 
\begin_inset CommandInset citation
LatexCommand cite
key "SimpleCV book"

\end_inset

, which lists many different examples and techniques to get me started.
\end_layout

\begin_layout Subsection
SIFT Descriptors
\end_layout

\begin_layout Section
Language tools
\end_layout

\begin_layout Standard
The language Haskell was used for the bulk of the coding because of a variety
 of tools that it brings to the project.
 These include simple concurrency through the use of STMs, a robust package
 management system and a powerful interpreter for testing individual modules.
 In this section we will go over each of these different areas and analyse
 what they offer to this project.
\end_layout

\begin_layout Paragraph
Standard Transactional Memory
\end_layout

\begin_layout Standard
Haskell comes packaged with a powerful method of concurrency, under the
 guise of the STM monad.
 The STM monad allows...
 
\begin_inset Newline newline
\end_inset

STM works by...
\begin_inset Newline newline
\end_inset

STM is more useful in this case because...
\end_layout

\begin_layout Paragraph
Cabal package management
\end_layout

\begin_layout Standard
Whilst creating a large project, it is unavoidable that many other modules
 will be imported along the way (unless one wishes to write everything themselve
s from scratch).
 The default package manager for Haskell, Cabal, manages all of this...
\begin_inset Newline newline
\end_inset

Cabal was picked instead of other package managers such as...
\end_layout

\begin_layout Paragraph
GHCi 
\end_layout

\begin_layout Standard
GHCi is the interpreter built into the Glasgow Haskell Compiler.
 It allows the user to dynamically load modules into the interpreter, automatica
lly locating dependencies and loading those too.
 The user can then use any of these loaded functions in the interpreter
 environment, creating data structures dynamically and testing the system.
 
\begin_inset Newline newline
\end_inset

In this project it was particularly useful for testing out various data
 structures and learning methods.
 For example, it was useful to be able to create a PID controller inside
 the interpreter environment, and then provide it with dummy values to see
 how the corresponding output would change with input.
 Whilst being beneficial for general prototyping, it was also useful for
 learning!
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Chapter
System Architecture
\end_layout

\begin_layout Standard
The helicopter control system is built from several different modules that
 interact with each other in a set manner.
 The modular structure is beneficial as it allows easy separation of responsibil
ities within the system, and also allows 'swapping' of certain modules with
 similar roles but different methods.
 The majority of the vision system is written in Python, the Arduino in
 (obviously) Arduino, and the control system in Haskell.
 Additionally, some of the debugging and simulation programs are written
 in Python.
\end_layout

\begin_layout Section
Crossing points
\end_layout

\begin_layout Standard
There are few key crossing points between different modules, where the different
 languages must cooperate with each other.
 These can roughly be seen in the diagram below:
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename Layout.png
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Simulations and control are all centralised to the Haskell 
\begin_inset Quotes eld
\end_inset

Majom
\begin_inset Quotes erd
\end_inset

 module.
 Control data is sent to the Helicopter via an Arduino device, which is
 communicated to through a serial port.
 Positional data is queried from the Vision module via a simple local HTTP
 server.
 Lastly, positional data from the simulation is sent straight to 
\series bold

\begin_inset Formula $stdout$
\end_inset


\series default
, which can be picked up directly by the Python simulation viewer.
\end_layout

\begin_layout Section
Modules
\end_layout

\begin_layout Standard
Each module has a defined set of tasks and responsibilities that it handles.
 These are briefly outlined below.
\end_layout

\begin_layout Subsubsection*
Vision
\end_layout

\begin_layout Standard
The vision system is written entirely in Python.
 Python was chosen for its extensive libraries, high level nature and ease
 of prototyping with its interpreter.
 Using the SimpleCV library, the module connects to the Kinect camera, and
 then combined with some computer vision techniques it obtains 3D positional
 and directional data of the Helicopter.
 This data is served up via a local HTTP server as needed by the control
 module.
\end_layout

\begin_layout Subsubsection*
Control
\end_layout

\begin_layout Standard
The control system is built in Haskell.
 The system uses a Flyer Type Class, which fully defines all of the required
 traits of something that is 
\begin_inset Quotes eld
\end_inset

flyable
\begin_inset Quotes erd
\end_inset

.
 Haskell was used for several reasons - it has a good amount of libraries,
 is strongly typed (useful for testing) and is (largely) pure.
 Additionally, I (the project writer) have personal experience developing
 AI in Haskell from previous projects.
\end_layout

\begin_layout Paragraph*
Flyable
\end_layout

\begin_layout Standard
A Flyable is a Type Class that must be able to receive orders, have an active/in
active status, and most importantly must be observeable.
 Concrete Types can become instances of the Flyable, such as the actual
 Helicopter itself (with observations coming from the Vision module), or
 a VirtualHelicopter (with observations coming directly from the simulator
 itself).
 Other, more complicated Flyables can also be defined, such as the DuoCopter
 which acts as an intermediary that allows two Controllers to fly one helicopter
 at once!
\end_layout

\begin_layout Paragraph*
Controller
\end_layout

\begin_layout Standard
Several different controllers have been defined, each interacting with the
 Helicopter in different ways.
 For instance, the GUI controller provides a method for the user to fly
 the Helicopter using the keyboard.
\begin_inset Newline newline
\end_inset

The main Controller of interest is the Monkey (the namesake of the project
 itself).
 The Monkey aims to learn the various variables that define how the Helicopter
 flies through reinforcement techniques, much like a human with some basic
 understanding of how a Helicopter flies would.
\end_layout

\begin_layout Subsubsection*
Arduino
\end_layout

\begin_layout Standard
The Arduino board receives information on what 'orders' to send to the helicopte
r via a serial cable.
 It works in cycles, receiving all new orders, then consolidating them into
 a final order set before sending this to the helicopter.
 It is written in the Arduino language, a derivative of Processing.
 It is a high level language that obscures a lot of the lower level working
 of the board itself, and come pre-packaged with the Arduino board itself.
\end_layout

\begin_layout Chapter
Execution
\end_layout

\begin_layout Standard
One of the key aims for the development of this project was to be able to
 remain flexible, and to quickly prototype new ideas and methods to see
 which one worked best for a given task.
 To achieve this, a solid 'framework' needed to be planned, assigning specific
 roles to specific areas, and ensuring that the abstract version would in
 fact work.
 
\end_layout

\begin_layout Section
Planning
\end_layout

\begin_layout Standard
To be able to properly develop in a flexible manner, it was important to
 lay out a good foundation to develop upon.
 First, the project was divided into distinct milestones, which should in
 theory be achieved in order, with each milestone providing a place to fall
 back on if a particular method does not work out.
 In the initial iteration, the milestones were
\end_layout

\begin_layout Itemize
Fly the helicopter manually from the computer
\end_layout

\begin_layout Itemize
Set up a basic simulation environment for the helicopter
\end_layout

\begin_layout Itemize
Track the helicopter in a simple fashion in a simplified environment
\end_layout

\begin_layout Itemize
Develop a basic controller that can fly the simulated controller
\end_layout

\begin_layout Itemize
Combine controller and simple vision to have a simple flyer
\end_layout

\begin_layout Itemize
Improve upon vision system to make it more robust
\end_layout

\begin_layout Itemize
Improve upon control system to allow for more complex instructions
\end_layout

\begin_layout Standard
As would be revealed as the project progressed, not all of these proved
 to be viable milestones.
\begin_inset Newline newline
\end_inset

In addition to having some set milestones, it was important to have a good
 skeletal structure for each module, especially so for the controller.
 This involved setting up contracts in the form of Type Classes for different
 parts of the controller, that each new test module would need to fulfill.
 This helped to realise the requirements for each module, and made testing
 much easier.
 For instance, the Flyable Type Class, which describes a 
\begin_inset Quotes eld
\end_inset

Flyable
\begin_inset Quotes erd
\end_inset

 object is written as:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Haskell,numbers=left,tabsize=2"
inline false
status open

\begin_layout Plain Layout

-- | Flyable class for flyable things that can fly.
\end_layout

\begin_layout Plain Layout

class Flyable a where
\end_layout

\begin_layout Plain Layout

  setFly :: a -> Option -> Int -> IO ()
\end_layout

\begin_layout Plain Layout

  setFlyMany :: a -> [(Option, Int)] -> IO ()
\end_layout

\begin_layout Plain Layout

  fly :: a -> IO ()
\end_layout

\begin_layout Plain Layout

  observe :: a -> IO (Power, Position)
\end_layout

\begin_layout Plain Layout

  isActive :: a -> IO Bool
\end_layout

\begin_layout Plain Layout

  setActive :: a -> Bool -> IO ()
\end_layout

\end_inset

Each function describes a responsibility for a Flyable object, that must
 be fulfilled.
 These include the key components of flying a helicopter, such as sending
 commands (
\begin_inset listings
lstparams "language=Haskell,tabsize=2"
inline true
status open

\begin_layout Plain Layout

setFly
\end_layout

\end_inset

) and observing the whereabouts of the helicopter (
\begin_inset listings
lstparams "language=Haskell,tabsize=2"
inline true
status open

\begin_layout Plain Layout

observe
\end_layout

\end_inset

), as well as more minor things such as classifying if a flyable is actually
 active.
 
\end_layout

\begin_layout Subsection
Arduino / Helicopter communication
\end_layout

\begin_layout Standard
One of the very beginning parts of the project was to acquire and set up
 an Arduino system for controlling the helicopter.
 As the whole project itself hinges on the helicopter being flown from the
 computer, this was an important foundation step.
 With some helpful tutorials
\begin_inset CommandInset citation
LatexCommand cite
key "Syma,Syma2"

\end_inset

, and online Arduino documentation
\begin_inset CommandInset citation
LatexCommand cite
key "Arduino,Arduino port"

\end_inset

 it was possible to get a first iteration of the helicopter control up and
 running from a laptop, and from there it was a simple case of trial and
 error to get a reliable connection to the helicopter.
 In the final iteration, the Arduino board waits on a serial connection
 from the computer, reads the incoming commands and then translates them
 into commands to relay to the helicopter.
 No response is expected from the helicopter itself.
\end_layout

\begin_layout Paragraph
Helicopter communication specifics
\end_layout

\begin_layout Description
Transmission
\begin_inset space ~
\end_inset

Method The Arduino communicates with the helicopter through an IR LED installed
 on the Arduino.
 The LED is pulsed on and off to represent the signal 
\begin_inset Quotes eld
\end_inset

on
\begin_inset Quotes erd
\end_inset

, which combined with periods of no signal give binary 0 and 1.
 A 
\begin_inset Quotes eld
\end_inset

1
\begin_inset Quotes erd
\end_inset

 bit is 320 microseconds on, 680 microseconds off, and a 
\begin_inset Quotes eld
\end_inset

0
\begin_inset Quotes erd
\end_inset

 bit is 320 microseconds on, 280 microseconds off.
\end_layout

\begin_layout Description
Packet
\begin_inset space ~
\end_inset

Measurements A packet consists of a header, the main order packet, and a
 footer.
 A header consists of 2000 microseconds on, then 2000 microseconds off.
 A footer consists of 300 microseconds on.
\begin_inset Newline newline
\end_inset

The main packet itself is 32 individual bits.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The first 8 bits (0-8) represent the Yaw value, with 63 being neutral, 0
 being full left yaw and 127 being full right yaw.
 
\end_layout

\begin_layout Itemize
The second 8 bits (8-16) represent the Pitch.
 Like Yaw, 63 is neutral, 0 is full back pitch and 127 is full forwards
 pitch.
\end_layout

\begin_layout Itemize
The third 8 bits (16-24) are the Throttle.
 This ranges from 0 (no throttle) to 127 (full throttle).
\end_layout

\begin_layout Itemize
The last 8 bits (24-32) are the Correction.
 Correction is a small amount of offset that can be specified manually to
 keep the helicopter from turning when the controller is in a neutral setting.
\end_layout

\end_deeper
\begin_layout Itemize
The structure of the packet can be easily seen from the Arduino code itself
 -
\begin_inset listings
lstparams "numbers=left,tabsize=2"
inline false
status open

\begin_layout Plain Layout

void SendCode() {
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  pulseIR(headerOn);
\end_layout

\begin_layout Plain Layout

  delayMicroseconds(headerOff);
\end_layout

\begin_layout Plain Layout

  pulseLength= headerOn + headerOff;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  for(int i=0; i < 32; i++) {
\end_layout

\begin_layout Plain Layout

    sendPulseValue(pulseValues[i]);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  //Footer
\end_layout

\begin_layout Plain Layout

  pulseIR(footerOn);
\end_layout

\begin_layout Plain Layout

  delayMicroseconds(messageLength - pulseLength - footerOn); 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Description
Communication
\begin_inset space ~
\end_inset

Loop A typical communication loop with Arduino involves:
\end_layout

\begin_deeper
\begin_layout Enumerate
Wait for incoming instructions from the serial port.
\end_layout

\begin_layout Enumerate
Filter the incoming instruction on instruction type (Yaw, Pitch, Throttle
 or Correction).
 Set the relevant bits in the new order packet based on these.
\end_layout

\begin_layout Enumerate
If there are no more incoming instructions, send the code using the protocol
 specified above.
 Otherwise, process new instructions and edit order packet accordingly.
\end_layout

\begin_layout Enumerate
Delay by 25 microseconds, then loop.
 This 25 microsecond delay prevented too many orders being sent to the helicopte
r in quick succession, thus confusing it.
\end_layout

\end_deeper
\begin_layout Section
Framework
\end_layout

\begin_layout Standard
An important part of the initial set up was developing suitable tools to
 make testing and development much easier.
 These could be best simplified to two parts - reliable communication methods
 between different modules, and powerful test tools to make prototyping
 and developing much easier.
\end_layout

\begin_layout Subsection
Communication methods
\end_layout

\begin_layout Standard
Some considerable thought was put into deciding what method should be used
 to link different language sections of the system.
 With such vastly different languages being used as Python and Haskell,
 it was no simple thing to just 
\begin_inset listings
lstparams "language=Python"
inline true
status open

\begin_layout Plain Layout

import monkey
\end_layout

\end_inset

 or 
\begin_inset listings
lstparams "language=Haskell"
inline true
status open

\begin_layout Plain Layout

import Vision
\end_layout

\end_inset

!
\begin_inset Newline newline
\end_inset

The main requirements for the communication method were simple; it needed
 to 
\end_layout

\begin_layout Itemize
Support the sending of a tuple/list of doubles, somewhere around the range
 of 4 or 5.
 This is to transmit the 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $y$
\end_inset

, 
\begin_inset Formula $z$
\end_inset

 and postional information from the vision system to the control system.
\end_layout

\begin_layout Itemize
Be fast.
\end_layout

\begin_layout Standard
Initially, we were hoping that it would be possible to use the Foreign Function
 Interface (FFI) 
\begin_inset CommandInset citation
LatexCommand cite
key "FFI"

\end_inset

 feature that comes with Haskell.
 However, it proved much more complicated to use than previously hoped,
 and it became evident early on that it would be wiser to search for alternative
 methods.
\end_layout

\begin_layout Subsubsection
Thrift
\end_layout

\begin_layout Standard
One method that was discovered was the software framework Apache Thrift
 
\begin_inset CommandInset citation
LatexCommand cite
key "Thrift"

\end_inset

.
 This seemed like a simple, effective and most of all flexible solution
 to the communication problem.
 With Thrift, it was possible to design C-like language-independent data
 structures, and then Thrift would provide all of the heavy lifting and
 translation between the two languages as required.
 For instance, for the simple relaying of positional information, the data
 structure definition was simply
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

struct Position {
\end_layout

\begin_layout Plain Layout

  1: double x,
\end_layout

\begin_layout Plain Layout

  2: double y,
\end_layout

\begin_layout Plain Layout

  3: double z
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

service Vision {
\end_layout

\begin_layout Plain Layout

  Position observe()
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset

where the struct is the data structure, and observe is the cross-language
 function call.
\end_layout

\begin_layout Standard
Communication between vision and monkey module (http server)
\end_layout

\begin_layout Standard
Communication between monkey and simulation module
\end_layout

\begin_layout Subsection
Testing tools
\end_layout

\begin_layout Paragraph
GUI for human control
\end_layout

\begin_layout Standard
One of the first and most basic of the testing tools that was developed
 was a GUI to let the user control the helicopter manually from the laptop.
 This served as a suitable test for the capabilities of the Arduino communicatio
n between the computer and the helicopter.
 Additionally, it enabled the system to output the specific values that
 were being sent to the helicopter, something that was not possible from
 flying the helicopter using the remote control.
 As detailed below, it also played a part in some other parts of the testing
 tools.
\end_layout

\begin_layout Paragraph
DuoCopter
\end_layout

\begin_layout Standard
After experimenting with some basic control methods for the helicopter early
 on in the project, it became obvious that it would be useful to restrict
 (and thus simplify) the parts of the helicopter that the controller can
 actually control.
 However, it would then be necessary for something else to control the rest
 of the helicopter, otherwise it might fly out of control whilst testing.
 To fulfill this purpose, a special 
\begin_inset Quotes eld
\end_inset

Duocopter
\begin_inset Quotes erd
\end_inset

 flyer was created.
 This Duocopter created two copies of the same helicopter control, and gave
 one to each controller.
 Incoming instructions from both of the controllers would be received by
 the Duocopter, filtered according order type, and then sent onwards to
 the helicopter itself.
 This process is invisible to both of the controllers and the helicopter.
\begin_inset Newline newline
\end_inset

For example, initially it was useful to let the Monkey control the Throttle
 of the helicopter, whilst stabilising and steering were controlled by a
 human operator through the GUI.
 The Duocopter would throw away any instruction received from the Monkey
 that was not a Throttle instruction, and likewise for non-Throttle instructions
 from the human.
 In this manner it was possible to test the Monkey Throttle control in a
 simplified environment without having to worry about the helicopter crashing!
\end_layout

\begin_layout Paragraph
Viewing methods
\end_layout

\begin_layout Standard
In addition with testing tools for controlling the helicopter, it was deemed
 useful to be able to 
\begin_inset Quotes eld
\end_inset

see what the controller sees
\begin_inset Quotes erd
\end_inset

, that is to have a method of visualising the data that is received by the
 Monkey when it tries to observe the helicopter.
 Original concepts for this revolved around outputting an image of the current
 observed position of the helicopter every iteration, and then making an
 animation out of the produced images.
 However, this lacked the real-time aspect that is required with most testing
 tools.
 These images could instead be displayed in a window created by the Monkey
 - the downside to this is that it is not overwhelmingly easy to do in Haskell,
 which reduces the ability to iterate on the design.
\begin_inset Newline newline
\end_inset

The idea chosen in the end was to develop it in Python - Python has many
 pre-installed modules that deal exactly with window output in a very simple
 and concise manner.
 Using the Pygame
\begin_inset CommandInset citation
LatexCommand cite
key "Pygame"

\end_inset

 module (which is already used in the SimpleCV module used for vision),
 creating a window of a specific size was as simple as 
\begin_inset listings
lstparams "language=Python,numbers=left,tabsize=2"
inline false
status open

\begin_layout Plain Layout

def main():
\end_layout

\begin_layout Plain Layout

  pygame.init()
\end_layout

\begin_layout Plain Layout

  window = pygame.display.set_mode((640,480))
\end_layout

\end_inset

From here, a simple two dimensional viewer was developed, one for a 
\begin_inset Quotes eld
\end_inset

side-on
\begin_inset Quotes erd
\end_inset

 perspective, showing 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 data, and another 
\begin_inset Quotes eld
\end_inset

top-down
\begin_inset Quotes erd
\end_inset

 perspective showing 
\begin_inset Formula $x,y,z$
\end_inset

 and 
\begin_inset Formula $orientation$
\end_inset

 data.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename presentTop.png
	width 15cm

\end_inset


\end_layout

\begin_layout Standard
It was desirable to make the connection between the Monkey (in Haskell)
 and the Python viewer as simple as possible, to reduce any potential mistakes.
 The resulting method was simply using 
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

stdin/stdout
\end_layout

\end_inset

 - the Monkey would print the positional information, and then by piping
 the output of one to the other in a Bash shell, the viewer would read the
 output, parse it and then display it.
 
\begin_inset Newline newline
\end_inset

This proved to be a simple yet effective manner for co-ordinating the two
 programs, with no need to worry about complicated cross-language tools
 or synchronisation problems, as all of the threading is handled by the
 Bash shell itself.
\end_layout

\begin_layout Paragraph
Simulation
\end_layout

\begin_layout Itemize
VirtualHelicopter
\end_layout

\begin_layout Itemize
Control variations
\end_layout

\begin_layout Itemize
Accuracy of the simulation
\end_layout

\begin_layout Section
Vision
\end_layout

\begin_layout Standard
The vision system is an integral part to the project - it is needed to track
 the helicopter in its surroundings, and then report the 3D position of
 the helicopter (and orientation) back to the controller.
 As discussed earlier, this has to be achieved with one stationary Kinect
 (RGB-D) camera, and a laptop.
 Additionally, the tracking method has to have sufficient granularity so
 that the controller can get updates in real time.
 This section will cover the various steps taken to achieve this, pitfalls
 that were encountered, and how the current system works.
\end_layout

\begin_layout Subsection
Initial requirements and decisions
\end_layout

\begin_layout Standard
At the beginning of the project, the following requirements were assembled
 that were deemed to be necessary for 'basic flight':
\end_layout

\begin_layout Enumerate
To be able to get a reasonable estimate for the 3D coordinates of the helicopter
 in a stationary environment.
\end_layout

\begin_layout Enumerate
To be able to estimate the orientation of the helicopter from some prior
 knowledge and the 'tracked' image of the helicopter.
\end_layout

\begin_layout Enumerate
To be able to do the above 2 things sufficiently quickly that the system
 can be used in a real-time situation.
\end_layout

\begin_layout Enumerate
The system should be sufficiently robust that 'unavoidable' interference
 (such as subtle lighting changes, shadows etc.) in the scene should not
 impair the tracking to any great degree.
\end_layout

\begin_layout Standard
With these requirements in mind, initial research was carried out into possible
 languages to implement the vision system in, and what libraries to use.
 
\begin_inset Newline newline
\end_inset

The first consideration was to use Haskell, as the main control system is
 written entirely in Haskell.
 This would make integration very simple.
 However, after further research it was discovered that vision libraries
 for Haskell are slim, and not particularly well supported.
 Most of the libraries are cross-language, partial implementations of the
 C++ OpenCV libraries.
 Due to their partial and experimental nature, and previous inexperience
 with OpenCV, it was deemed too risky to attempt to proceed with the vision
 module in Haskell.
\begin_inset Newline newline
\end_inset

The second consideration was using C++ and OpenCV.
 This was an obvious consideration, as huge amounts of computer vision research
 is done in OpenCV and C++, with the language/library pair being considered
 the 'main' computer vision method.
 However, this approach was not entirely without flaw - inexperience with
 both C++ and vision in general made this quite daunting.
 Whilst a good choice if no other alternative are found, it seemed that
 a further search for a simpler language/implementation might prove to be
 beneficial.
\begin_inset Newline newline
\end_inset

The last consideration was the SimpleCV library, which is written in Python.
 The SimpleCV uses the Python implementation of OpenCV, but provides a large
 amount of 'simple' methods and data structures that hide the more complex
 data structures below.
 Additionally, it is well supported with many tutorials and examples.
 This seemed to fulfill the most requirements without many downsides, so
 development began using that.
\end_layout

\begin_layout Subsection
Basic Software and Setup
\end_layout

\begin_layout Standard
Before prototyping could begin, it was necessary to find pre-existing software
 packages that would perform a lot of the 'heavy lifting' of hardware management
 with the Kinect.
 Fortunately, all of this was well described by the SimpleCV documentation
 - for hardware drivers, the open source Kinect library OpenKinect (via
 freenect) 
\begin_inset CommandInset citation
LatexCommand cite
key "freenect website"

\end_inset

 was used.
 This provided basic Kinect functions for Python.
 The SimpleCV library is then built on top of this, adding extra functions
 for general image processing and video processing.
 Finally, it was necessary to calibrate the Kinect, which was also simple
 given the provision of a tutorial for calibration in the freenect package.
\end_layout

\begin_layout Subsection
RBG Vision
\end_layout

\begin_layout Standard
The first method that was tested for performing the position detection was
 based around using the RGB camera for initial detection, and using the
 depth camera to 'augment' the detection for accuracy and depth measurements.
 The founding for this idea was that the RGB camera was less noisy than
 the depth camera, which was prone to casting depth 'shadows', areas where
 no depth information could be recovered.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename shadow_example.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

Figure showing the appearance of 'depth-shadows', visualised as white areas
 in the image.
 The Kinect device is incapable of obtaining depth information these areas.
 These shadows are a by-product of the depth-detection method, and unfortunately
 cannot be avoided using the Kinect 'as-is'.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Groundwork
\end_layout

\begin_layout Standard
Following the assumptions stated in 5.3.1, an idea to initially locate the
 helicopter was to take one image of the static scene without the helicopter
 present, and then to subtract this base image from the new images that
 were being taken.
 As the helicopter was assumed to be the only moving object in the scene,
 this should in theory leave just the shape of the helicopter, which could
 be refined using standard imaging techniques.
 Having successfully segmented the helicopter from the rest of the scene,
 it was now necessary to work out the corresponding location of the helicopter
 in the depth map.
 This, however, was where the problems began.
 Without the depth information of the helicopter, it was impossible (or
 at least, very difficult) to reconstruct the position of the helicopter
 into three-dimensions - and without this 3D projection, it was not possible
 to project it again into the depth camera simply by using the RGB camera
 alone.
 
\begin_inset Newline newline
\end_inset

Several methods were tried to remedy this:
\end_layout

\begin_layout Description
Best-guess
\begin_inset space ~
\end_inset

search One method that was tried was locating the position of the helicopter
 in the RGB camera, getting shape information of the helicopter, and then
 trying to again locate this shape (or similar shapes) in an area that would
 approximately match that on the depth camera.
 Drawbacks of this method included additional overhead of doing image analysis
 twice, difficulty locating similar shapes in both cameras (as each camera
 had a different 'type' of error that it would pick up) and being somewhat
 overcomplicated.
\end_layout

\begin_layout Description
Estimating
\begin_inset space ~
\end_inset

camera
\begin_inset space ~
\end_inset

parameters Another naive method used was just to 'guess' where the helicopter
 would be in the depth camera from ad-hoc observations about relative positions
 in the depth and RGB images.
 This proved to not be accurate enough at all, and upon later reflection
 to be a bit of a waste of time.
 
\end_layout

\begin_layout Standard
It seemed that short of using the depth camera purely to locate the helicopter,
 it would be very difficult to recreate the helicopter in three dimensions
 using the RGB camera.
 So that is exactly what we tried to do next (Section 5.3.4).
\end_layout

\begin_layout Subsubsection
Prototyping results
\end_layout

\begin_layout Itemize
Stable lighting problems
\end_layout

\begin_layout Itemize
Shadows
\end_layout

\begin_layout Itemize
Similar colours
\end_layout

\begin_layout Subsection
D Vision
\end_layout

\begin_layout Standard
Having tested out the RGB camera methods, the next idea was to try to recreate
 the best parts of the RGB detection using just the depth camera, whilst
 trying to minimise the downsides of the depth camera itself (as mentioned
 earlier, one of the main problems was excessive noise in depth images caused
 by 'depth-shadowing').
 
\end_layout

\begin_layout Subsubsection
Groundwork
\end_layout

\begin_layout Standard
There were a few differences between the depth camera and the RGB camera
 that needed to be considered before development could be started on the
 depth detection.
\begin_inset Formula 
\[
\begin{array}{ccc}
 & \mbox{RGB} & \mbox{D}\\
Noise & \mbox{Not noisy} & \mbox{Noisy}\\
Dimensions & (0-255)^{3} & (0-2047)\\
Errors & \mbox{No} & \mbox{Yes}\\
Spectrum & \mbox{Visible light} & \mbox{IR light}
\end{array}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Noise
\end_layout

\begin_layout Standard
One of the main benefits of the RGB images was that they were not overly
 prone to noise - as long as the objects in the scene remained stationary,
 two consecutive images would be essentially identical.
 The depth images, by comparison, were very prone to noise as the depth-shadows
 moved in the scene.
 This meant that an otherwise stationary scene would still cause noise in
 a naive subtraction detection algorithm.
\end_layout

\begin_layout Paragraph
Dimensions
\end_layout

\begin_layout Standard
The RGB camera operates with 3 dimensions of values (as the name suggests)
 - 0-255 for each of Red, Green and Blue.
 This means that there is a total of 16581375 different values for each
 pixel.
 The depth camera, by contrast, only takes values from 0-2047, with 2047
 being the error value.
 Whilst this may seem that the detail provided is significantly less, in
 practise this does not affect the ability of the camera to detect the helicopte
r to any noticeable degree.
\end_layout

\begin_layout Paragraph
Errors
\end_layout

\begin_layout Standard
Every pixel on an RGB image was valid - the closest that one could get to
 an error value on an RGB image was a 'too light' or 'too dark' area, which
 would manifest themselves as the values 0 or 255 respectively.
 For the depth images, this was also true - too close values would appear
 as 0, and too far values would appear as 2046, however in addition areas
 that could not be properly reconstructed would appear as 2047, the error
 value.
\end_layout

\begin_layout Paragraph
Spectrum
\end_layout

\begin_layout Standard
This is a fairly obvious difference - the RGB camera operates purely over
 visible light, whereas the depth camera operated using IR light.
 This was the key factor that set the depth camera apart from the RGB camera
 - it meant that it was not sensitive to changing lighting conditions, or
 to shadows cast by objects or the helicopter.
\end_layout

\begin_layout Subsubsection
Prototyping results
\end_layout

\begin_layout Subsection
Orientation Detection
\end_layout

\begin_layout Standard
The second vital part of the helicopter detection is finding out which way
 the helicopter is facing - without this knowledge, it is impossible to
 steer the helicopter in any particular direction.
 Orientation detection presented an interesting challenge, and was probably
 the more complicated part of the Vision module.
 This complexity was exaggerated by the low resolution of the RGBD images
 - both of which operate at 640x480 pixels.
 At any one point, it is unlikely that the helicopter would take up more
 than a 100x100 pixel square in the original image!
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename r_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
An example image of the helicopter after basic position detection.
 Basic detail is obtained, but the distinction between helicopter and background
 is less obvious on the metal chassis and on the blades themselves.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Groundwork
\end_layout

\begin_layout Standard
From the previous work with helicopter detection, it was obvious that using
 the depth camera to also perform orientation analysis would not be sufficient.
 The reflective property of the helicopter, combined with its small size
 and the low fidelity of the depth image meant that a meaningful capture
 of the helicopter would be too difficult.
 Instead, we decided to focus on using the RGB camera.
 Figure 5.2 appears to show that we can get enough information from a well-lit
 RGB image to discern the orientation.
\end_layout

\begin_layout Paragraph
Naive extraction
\end_layout

\begin_layout Standard
As an initial stepping stone, a basic extraction method using image subtraction
 was used.
 This was a two-step process - given a 'base' image of the scene, without
 the helicopter, and another with the helicopter in it:
\end_layout

\begin_layout Enumerate
Take the absolute difference between the two images to obtain a greyscale
 'difference' image.
\end_layout

\begin_layout Enumerate
Perform image thresholding on the resulting image, keeping values that are
 different enough and discarding the rest (being regarded as image noise).
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python,tabsize=2"
inline false
status open

\begin_layout Plain Layout

# Get the numpy array of values
\end_layout

\begin_layout Plain Layout

n1 = base.getNumpy() 
\end_layout

\begin_layout Plain Layout

n2 = img.getNumpy()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Take the absolute difference between them
\end_layout

\begin_layout Plain Layout

diff = Image(absdiff(n1,n2))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Perform basic thresholding and erosion to get 
\end_layout

\begin_layout Plain Layout

# rid of noise and leave a black and white mask
\end_layout

\begin_layout Plain Layout

mask = diff.binarize().erode()
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This gave a reasonably good depiction of the outline of the helicopter as
 can be seen in Figure 5.3.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename h_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

Extraction of the helicopter from the image in Figure 5.2, using the subtractive
 method.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Signal decomposition
\end_layout

\begin_layout Standard
One suggested method of orientation analysis was to decompose the silhouette
 of the helicopter into height along the y-axis.
 The basis of this was that the front of the helicopter has more 'stuff'
 to it - it is taller and rounder than the tail of the helicopter, which
 is comparitively small.
 Thus the decomposition would result in a graph that is taller at one end
 and shorter at the other, and the distance between these two features would
 discern the orientation of the helicopter.
\begin_inset Newline newline
\end_inset

Initial tests with this method were not particularly conclusive - a clear
 peak can be 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename g_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

Initial horizontal signal decomposition of the helicopter.
 A peak is clearly present on the left hand side, comprised of the two helicopte
r blades and the main body of the helicopter.
 
\end_layout

\end_inset

seen for the two helicopter blades and the main body of the helicopter.
 However, it is unclear where the body of the helicopter ends - this is
 caused by the fact that the helicopter blades take up a large portion of
 the length of the helicopter.
 Another problem that they cause is they are quite noisy - their detected
 position in any subsequent images may be different and thus subsequently
 generated graphs would also be quite noisy.
\begin_inset Newline newline
\end_inset

To attempt to remedy this, a simple method was devised to remove the helicopter
 blades from the scene.
 The helicopter always has two large shadows, one for each helicopter blade.
 By looking at the horizontal scanlines, rather than the vertical scanlines
 (Figure 5.5) it is possible to see that these are clearly identifiable as
 the first two 'large' peaks.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename gv_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

Vertical decomposition of the helicopter signal.
 The two large peaks on the left are caused by the two main blades of the
 helicopter.
\end_layout

\end_inset

By cropping the portion of the image that holds these two peaks, (achived
 by searching linearly from the left until two 'large' peaks are met.
 The exact definition of large is relative to the rest of the signal) it
 is possible to remove the blades from the image entirely, providing the
 clearer horizontal decomposition in Figure 5.6.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename gh_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

New horizontal decomposition, with helicopter blades removed.
 Now two entities are clearly visible - the large cockpit of the helicopter
 on the left, and the smaller tail rotor on the right.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Sift descriptor
\end_layout

\begin_layout Standard
After discussing the above findings with the supervisor, it was decided
 that it might be beneficial to try an alternate approach, using a 'Sift
 Descriptor'.
\end_layout

\begin_layout Subsection
Orientation Inferrence
\end_layout

\begin_layout Standard
Another, simpler, method of orientation analysis attempted was inferrence.
 This worked on the underlying assumption that as the helicopter is incapable
 of 'strafing' motion (that is motion to the left or right whilst facing
 forwards), provided there is some forwards pitch the helicopter will move
 in the direction it is facing.
 Taking an average of the 
\begin_inset Formula $N$
\end_inset

 most recent velocities from the 
\begin_inset Formula $N+1$
\end_inset

 positions recorded dampens most of the observational noise, and provides
 a fairly accurate representation of the current heading - larger values
 of 
\begin_inset Formula $N$
\end_inset

 will be slower to react to velocity changes, yet will be more resilient
 to noise.
 These velocities are averaged, and the heading is then extracted (in our
 case, values 0 - 359 are used, with 0 being directly away from the camera,
 increasing clockwise).
\end_layout

\begin_layout Subsubsection
Rough results
\end_layout

\begin_layout Section
Assembly
\end_layout

\begin_layout Itemize
Problems integrating the two languages
\end_layout

\begin_deeper
\begin_layout Itemize
Approaches used, tested, thrown out
\end_layout

\begin_layout Itemize
Thrift
\end_layout

\begin_layout Itemize
HTTP server instead (why, what it fulfills)
\end_layout

\end_deeper
\begin_layout Itemize
Initial observations
\end_layout

\begin_deeper
\begin_layout Itemize
Show noise with/without filters
\end_layout

\end_deeper
\begin_layout Itemize
Improvements, filters etc.
 
\end_layout

\begin_layout Section
Control
\end_layout

\begin_layout Standard
Accurate detection was only one of the requirements that needed to be fulfilled
 - armed with positional data, it was now important to decide how to control
 the helicopter itself.
 Fortunately, the design of the helicopter itself allows for conceptual
 simple controls for a human controller, and as such it is (relatively)
 simple to automate also.
 
\begin_inset Newline newline
\end_inset

The control of the helicopter can be split into two separate areas with
 minimal overlap.
 The first of these is the vertical motion of the helicopter, governed by
 the main rotors and the throttle - increasing the throttle of the helicopter
 results in an increase in vertical velocity and vice versa for decreasing
 throttle.
 The second area is motion in the 
\begin_inset Formula $x-z$
\end_inset

 plane, which is controlled by the pitch and yaw of the helicopter.
 Due to the simplistic design, the helicopter is incapable of deliberate
 strafing motion left and right (of course it is still susceptible to sideways
 drift from air currents or its own momentum), so navigation is accomplished
 by pointing the helicopter in the desired direction, then changing the
 pitch until the desired velocity is acquired.
 These two areas are mostly disjoint, however there is still a small amount
 of overlap.
 The degree of forward motion provided by a change of pitch is increased
 accordingly with the amount of throttle currently being used, and corresponding
ly the upwards velocity is decreased as the pitch increases (as upwards
 velocity becomes a diagonal-forward velocity.
 The decrease is proportional to 
\begin_inset Formula $\cos\theta$
\end_inset

, where 
\begin_inset Formula $\theta$
\end_inset

 is the angle between the 
\begin_inset Formula $y-axis$
\end_inset

 and the upward vector of the helicopter).
 
\begin_inset Newline newline
\end_inset

Based on these observations, it is possible to construct several different
 control methods.
\end_layout

\begin_layout Subsection
Control methods
\end_layout

\begin_layout Itemize
How helicopter will be controlled
\end_layout

\begin_layout Itemize
Started considering how a human does it
\end_layout

\begin_layout Itemize
PID
\end_layout

\begin_layout Paragraph
Intentions
\end_layout

\begin_layout Paragraph
Enacting Intentions
\end_layout

\begin_layout Itemize
Simple point and drive
\end_layout

\begin_layout Itemize
Combinations
\end_layout

\begin_layout Chapter
Evaluation
\end_layout

\begin_layout Paragraph
Required functionality
\end_layout

\begin_layout Enumerate
Automated flight with no input other than camera.
\end_layout

\begin_layout Enumerate
Input of 'intentions', and automatic execution of them.
\end_layout

\begin_layout Enumerate
Movement in one, two and three dimensions.
\end_layout

\begin_layout Standard
I believe that these three points establish the 'baseline' of the functionality
 I should achieve.
 Successful execution of these should demonstrate complexity in computer
 vision, AI and general design, and should be sufficiently challenging to
 perform.
 
\end_layout

\begin_layout Section
Quality of functionality
\end_layout

\begin_layout Standard
It should prove to be quite simple to ascertain the quality of the functionality
:
\end_layout

\begin_layout Description
Observation It will be fairly evident from how the helicopter flies according
 to the given instructions how well the project is performing.
 For instance, a steady, certain flight path would be good whereas a juddering
 path, or even crashing, would indicate bad performance!
\end_layout

\begin_layout Description
Execution The ability to execute more and more complicated intentions.
 Simply hovering in a given position should prove to be a (comparatively)
 simple feat, whereas flying from one position to another is more demanding.
 Extending this to customisable paths in three dimensional space would show
 that the underlying mechanisms are of high quality and performance.
\end_layout

\begin_layout Description
Extensibility If the system is set up well, it should be easy to extend
 it with new abilities and features, using the existing functionality.
\end_layout

\begin_layout Standard
These could be measured using the following demonstration, for example:
\end_layout

\begin_layout Enumerate
Demonstrate the ability to fly the helicopter from the computer manually.
 
\shape italic
Demonstrates the foundations of the project.
 
\end_layout

\begin_layout Enumerate
Demonstrate the ability to hover the helicopter in place, even when displaced.
 
\shape italic
Demonstrates the static control of the helicopter from a single camera source
 in multiple dimensions.
 
\end_layout

\begin_layout Enumerate
Demonstrate the ability to move in a predefined path.
 
\shape italic
Demonstrates advanced functionality.
 
\end_layout

\begin_layout Section
Qualitative analysis of the system
\end_layout

\begin_layout Standard
To evaluate the limitations of the system, several tests were set up.
 These tests were designed to quantitavely see the exact capabilities and
 limiations of the system.
 Qualities tested (split into traits) were:
\end_layout

\begin_layout Paragraph
Position detection (vision)
\end_layout

\begin_layout Itemize
Range of detection
\end_layout

\begin_layout Itemize
Robustness of detection
\end_layout

\begin_layout Itemize
Accuracy of detection
\end_layout

\begin_layout Itemize
Limitations
\end_layout

\begin_layout Paragraph
Orientation detection (vision)
\end_layout

\begin_layout Itemize
Accuracy of detection
\end_layout

\begin_layout Itemize
Limitations
\end_layout

\begin_layout Paragraph
System properties
\end_layout

\begin_layout Itemize
Helicopter response times
\end_layout

\begin_layout Itemize
Multiple signals
\end_layout

\begin_layout Itemize
Control range/robustness (Arduino)
\end_layout

\begin_layout Itemize
Problems with the system
\end_layout

\begin_layout Paragraph
Helicopter properties
\end_layout

\begin_layout Itemize
Speeds of helicopter
\end_layout

\begin_layout Itemize
Flight time of helicopter
\end_layout

\begin_layout Itemize
Range of motion of helicopter
\end_layout

\begin_layout Section
Achievements
\end_layout

\begin_layout Section
Strengths and weaknesses of the system
\end_layout

\begin_layout Section
Comparison to existing technology
\end_layout

\begin_layout Chapter
Conclusions
\end_layout

\begin_layout Section
Learning results of this project
\end_layout

\begin_layout Section
Future work
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Syma"

\end_inset

Lee, J.
 (2011) Procrastineering - Project blog for Johnny Chung Lee: Computer Controlli
ng a Syma Helicopter.
 [online] Available at: http://procrastineering.blogspot.co.uk/2011/11/computer-con
trolling-syma-helicopter.html [Accessed: 1 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Syma2"

\end_inset

Avergottini.com (2011) Couch Sprout: Arduino helicopter infrared controller.
 [online] Available at: http://www.avergottini.com/2011/05/arduino-helicopter-infr
ared-controller.html [Accessed: 1 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Arduino"

\end_inset

Arduino.cc (n.d.) Arduino - Guide.
 [online] Available at: http://arduino.cc/en/Guide/Guide [Accessed: 1 Jan
 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "IR"

\end_inset

Ladyada.net (2012) Sensor tutorials - IR remote receiver/decoder tutorial.
 [online] Available at: http://www.ladyada.net/learn/sensors/ir.html [Accessed:
 1 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Arduino port"

\end_inset

Arduino.cc (n.d.) Arduino - PortManipulation.
 [online] Available at: http://www.arduino.cc/en/Reference/PortManipulation
 [Accessed: 1 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "freenect paper"

\end_inset

Conley, K.
 (2012) Adding Video Support to Christopher Doneâs Haskell Kinect Library.
 [e-book] http://static.kevintechnology.com/docs/cis194.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "freenect"

\end_inset

kevincon (2012) freenect.
 [online] Available at: https://github.com/kevincon/freenect [Accessed: 4
 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Least squares + Recursive"

\end_inset

Unknown.
 (2010) Recursive Least Squares Estimation.
 [e-book] Available through: http://www.cs.iastate.edu http://www.cs.iastate.edu/~cs57
7/handouts/recursive-least-squares.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Recursive least squares"

\end_inset

Unknown.
 (n.d.) Lecture 10: Recursive Least Squares Estimation.
 [e-book] Available through: http://www.cs.tut.fi http://www.cs.tut.fi/~tabus/course/A
SP/LectureNew10.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Least squares wiki"

\end_inset

En.wikipedia.org (2012) Least squares - Wikipedia, the free encyclopedia.
 [online] Available at: http://en.wikipedia.org/wiki/Least_squares [Accessed:
 4 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "OpenCV book"

\end_inset

BRADSKI, G.
 R., & KAEHLER, A.
 (2008).
 Learning OpenCV: computer vision with the OpenCV library.
 Sebastopol, CA, O'Reilly.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "OpenCV examples"

\end_inset

aleator (2012) CV.
 [online] Available at: https://github.com/aleator/CV/tree/master/examples
 [Accessed: 5 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "SLAM"

\end_inset

En.wikipedia.org (2005) Simultaneous localization and mapping - Wikipedia,
 the free encyclopedia.
 [online] Available at: http://en.wikipedia.org/wiki/Simultaneous_localization_and
_mapping [Accessed: 7 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Camera based nav of low cost quadropter"

\end_inset

J.
 Engel, J.
 Sturm, D.
 Cremers (2012) Camera-Based Navigation of a Low-Cost Quadrocopter .
 [e-book] Available through: http://vision.in.tum.de/research/quadcopter http://vis
ion.in.tum.de/_media/spezial/bib/engel12iros.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Autonomous Camera based quadropter"

\end_inset

Engel, J.
 (2011) Autonomous Camera-Based Navigation of a Quadrocopter .
 Masters.
 Der Technischen Universitat Munchen.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Kinectfusion"

\end_inset

Izadi, S.
 et al.
 (n.d.) KinectFusion: Real-time 3D Reconstruction and Interaction Using a
 Moving Depth Camera*.
 [e-book] Available through: http://research.microsoft.com/ http://research.microso
ft.com/pubs/155416/kinectfusion-uist-comp.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Odometry quadropter"

\end_inset

Kerl, C.
 (2012) Odometry from RGB-D Cameras for Autonomous Quadrocopters .
 [e-book] Available through: http://vision.in.tum.de/research/quadcopter http://vis
ion.in.tum.de/_media/spezial/bib/kerl2012msc.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Kalman wiki"

\end_inset

En.wikipedia.org (1960) Kalman filter - Wikipedia, the free encyclopedia.
 [online] Available at: http://en.wikipedia.org/wiki/Kalman_filter [Accessed:
 16 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "kalman tutorial"

\end_inset

Cs.unc.edu (2012) The Kalman Filter.
 [online] Available at: http://www.cs.unc.edu/~welch/kalman/ [Accessed: 16
 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "SimpleCV book"

\end_inset

DEMAAGD, K., OLIVER, A., & OOSTENDORP, N.
 (2012).
 Practical computer vision with SimpleCV.
 Beijing, O'Reilly.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Pygame"

\end_inset

Pygame.org (2012) News - pygame - python game development.
 [online] Available at: http://www.pygame.org [Accessed: 25 Apr 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "PID Wiki"

\end_inset

En.wikipedia.org (1890) PID controller - Wikipedia, the free encyclopedia.
 [online] Available at: http://en.wikipedia.org/wiki/PID_controller [Accessed:
 20 May 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "PID Tuning"

\end_inset

Zhong, J.
 (2006) PID Controller Tuning: A short tutorial.
 [e-book] http://wwwdsa.uqac.ca/~rbeguena/Systemes_Asservis/PID.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "freenect website"

\end_inset

Openkinect.org (2000) OpenKinect.
 [online] Available at: http://openkinect.org/wiki/Main_Page [Accessed: 21
 May 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "FFI"

\end_inset

Haskell.org (n.d.) 8 Foreign Function Interface.
 [online] Available at: http://www.haskell.org/onlinereport/haskell2010/haskellch8.
html [Accessed: 29 May 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Thrift"

\end_inset

Thrift.apache.org (2012) Apache Thrift.
 [online] Available at: http://thrift.apache.org/ [Accessed: 29 May 2013].
\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Appendix
\end_layout

\begin_layout Section*
User Guide
\end_layout

\begin_layout Itemize
How to use the helicopter flight system
\end_layout

\end_body
\end_document
