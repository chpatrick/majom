#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass report
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 4cm
\topmargin 3cm
\rightmargin 4cm
\bottommargin 4cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Report
\end_layout

\begin_layout Author
Luke Tomlin
\end_layout

\begin_layout Date
04/03/13
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Abstract
This project details the development and results of a system that autonomously
 flies a small remote-control helicopter in an unknown environment.
 The only measurement data is obtained from a standard Kinect Camera, and
 this data is used to make assumptions on the flight properties of the helicopte
r, it's surroundings, and then fly the helicopter accordingly.
 This is an exercise in performing a task that requires a high degree of
 prior knowledge and information, using cheap and (comparitively) simple
 hardware.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Subsection*
Acknowledgements
\end_layout

\begin_layout Standard
Thanks to the help I got from my tutor, Andrew Davison.
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
As modern technology increases in power and decreases in size, interest
 in autonomous control of micro-aerial vehicles (MAV) has increased.
 What was not always accessible to casual users and non-specialized researchers
 has suddenly become a lot more accessible - small aerial radio controlled
 vehicles are now cheaper and more capable than ever before.
 Taking this to an almost extreme, in this project we endeavour to perform
 indoor MAV autonomous piloting (in this particular case, a small RC helicopter)
, using cheap widely available hardware and free, opensource software.
\end_layout

\begin_layout Standard
The problem of automating remote-controlled helicopter flight is best described
 by the limiting factors - there is a room indoors, a small remote-controlled
 helicopter, a computer and a simple camera source capable of estimating
 depth to some degree of accuracy (also known as an RGB-D camera).
 All of the individual parts are off the shelf, general products that are
 purchaseable by the general public at low-cost.
 This is good, and bad - whilst they may be afforadable and accessible,
 they are not tailored for this specific problem and as such are prone to
 errors and noise.
 The challenge of this project arises from these inaccuracies; pinpointing
 them and working around them.
 This is an interesting problem of some merit, as any system devised under
 these conditions should be easily replicated by anyone without the requirement
 of any specialised equipment.
\end_layout

\begin_layout Standard
The simple RGB-D camera that will be used in this project is Kinect.
 It is capable of generating real-time depth maps, which represent the depth
 of the points visible to the camera.
 This data can be viewed as a greyscale image (scaling the depth values
 from 0 to 255) or can be extrapolated into the 3D environment using the
 information about camera.
 These readings are not always accurate, and may fluctuate in a seemingly
 motionless scene.
 In addition to the depth camera on board the Kinect, it has a simple RGB
 camera capable of recording in 640x480.
\begin_inset Newline newline
\end_inset

In addition to the Kinect, all other devices and software are generic and
 easily obtainable, all costing under Â£30 (excluding the computer).
 This includes the communication mechanism between the helicopter and the
 computer, and even the helicopter itself.
\begin_inset Newline newline
\end_inset

The aim of this report is to document the iterative process of the project,
 discussing methods attempted and their results, and then describing the
 finished system as a whole, its construction and its capabilities.
 In this chapter, we will give a brief overview of the technology used as
 well as the general outlook for the project itself.
 In Chapter 2, we discuss other similar projects and analyse what we can
 use as inspiration for our research and prototyping.
 Chapter 3 describes the final structure of the system, with each modules
 development being discussed in Chapter 4.
 A comprehensive testing of the system's basic properties and its effectiveness
 is listed in Chapter 5.
 For a step-by-step demonstration of the system in action, please look at
 the appendix at the end of this report.
\end_layout

\begin_layout Section
Deconstruction
\end_layout

\begin_layout Standard
The structure of the automation system can be easily understood by separating
 the individual requirements of the system itself.
 Once it has been abstracted into separate problems, these can be researched
 and worked on in a modular fashion, creating tools and sub-systems with
 more refined responsibilities.
 For instance, the computer vision problem is entirely separate from that
 of the control system - the control operates under the assumption that
 the positional information is mostly accurate, and it is up to the computer
 vision module to provide these mostly accurate readings.
 For simplicity, the communication links between these modules should be
 as basic as possible.
\begin_inset Newline newline
\end_inset

The driving idea behind this deconstruction is considering how a human would
 fly the helicopter, reducing it to the key problems and then considering
 how it could be formulated into a programming problem.
 The final system will then mirror this deconstruction, each module providing
 a different role.
\end_layout

\begin_layout Paragraph
Computer Vision
\end_layout

\begin_layout Standard
There are a multitude of tasks that need to be performed from a vision perspecti
ve before the system can even begin to function.
 These include segmentation of the image to locate the helicopter, tracking,
 spacial analysis and analysis of the helicopter itself (to get the current
 orientation, for instance).
 These cover a range of difficulties, and cover a range of 'features' for
 the system, from simple hovering to fully automated flight.
\end_layout

\begin_layout Paragraph
Control Mechanism
\end_layout

\begin_layout Standard
The program that flies the helicopter is wholly separate from the vision,
 and instead is supposed to abstract the fundamental properties for flying
 the helicopter into a complete control method.
 The main requirement, given information about the current whereabouts and
 properties of the helicopter, is to be able to issue commands intelligently
 to the helicopter and pilot it.
 Additionally, there should be a well-defined, expressive, and expandable
 method for defining piloting behaviour, such as hovering in place, flying
 to a specific point, or following a predefined path.
\end_layout

\begin_layout Section
Technology
\end_layout

\begin_layout Standard
In this section we will list some of the technology that is used in the
 system.
 This includes hardware for vision and communication, as well as software
 libraries used.
\end_layout

\begin_layout Subsection
Kinect
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kinect.png
	display false
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
For the computer vision, we will use the Kinect sensor device, created by
 Microsoft, which provides both an RGB camera and a method for calculating
 depth of the image (through an infra-red camera).
 Additionally, there are several freely available open-source software kits
 for interacting with the Kinect on any platform.
 This simplifies the computer vision aspect somewhat, allowing the direct
 extraction of the 3D coordinates instead of through more traditional means
 such as stereoscopic vision.
 Additionally, substantial experimentation has gone into using it for computer
 vision previous - providing a wealth of prior experience to better gauge
 the possibilities and limitations of using the Kinect as part of the vision
 system.
\end_layout

\begin_layout Subsection
OpenCV
\end_layout

\begin_layout Standard
OpenCV (Open Source Computer Vision Library) is a library of programming
 functions that can be used to perform image analysis from the Kinect.
 It is free to use, and has many different implementations in many different
 programming languages.
 The specific version of OpenCV used in this project uses the Python bindings
 in a library called SimpleCV, which provides some additional functions
 in addition to those normally supplied.
 These will be covered in more detail later on.
\end_layout

\begin_layout Subsection
Arduino
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename arduino.png
	display false
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
The Arduino
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://arduino.cc
\end_layout

\end_inset


\end_layout

\end_inset

 is an open-source microcontroller, which lets the user work with a simple
 programming language (C-like) to control how the outputs work.
 It also comes with a Processing-based IDE for programming in, that handles
 a lot of the communications between the board and the computer out of the
 box.
 Additionally, the board itself is reasonably cheap to purchase and highly
 re-usable.
 
\begin_inset Newline newline
\end_inset

In this project we used the Arduino Uno
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://arduino.cc/en/Main/arduinoBoardUno
\end_layout

\end_inset

 for more information
\end_layout

\end_inset

 - it provides 14 separate digital input/output pins and a USB connection,
 along with many other features that are outside the scope of this project.
 It is powered directly from the computer through the provided USB cable.
 Additionally it provides a hard reset button on the board itself, which
 proved to be invaluable during the flight prototyping phase.
\end_layout

\begin_layout Subsection
Syma Remote Control Helicopter
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename syma.png
	display false
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
The Syma Remote Control Helicopter is a small indoors-flying helicopter,
 that is controlled by an IR signal sent from a remote control.
 It is modelled to be easy to fly indoors, and to remain stable even in
 in-experienced hands.
 It does this by not having a traditional tail rotor - instead, it has an
 internal gyroscope that keeps it steady, and rotation is accomplished by
 speeding up and slowing down the main rotors.
 The tail rotor is instead positioned facing upwards, and provides a tilting
 force to the tail, letting the helicopter fly forwards and backwards.
 This makes the helicopter extremely stable, at the sacrifice of not being
 able to tilt left or right.
 Ultimately, this only serves to make it easier to fly.
 The details on the communication protocols of the helicopter are not freely
 available, but some third party sources have done their own analysis of
 the signals (using oscilloscopes for example), and this should prove to
 be sufficient starting point.
 These are discussed later.
\end_layout

\begin_layout Chapter
Background
\end_layout

\begin_layout Standard
The idea of this project originates from a suggestion by Google X engineer
 Johnny Chung Lee, from his blog post 
\begin_inset Quotes eld
\end_inset

Computer Controlling a Syma Helicopter
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Syma"

\end_inset

.
 In his post, he talks about possible methods for setting up the control
 of a remote helicopter, as far as controlling it manually from a computer.
 He then conjectures the possibility of using a camera or similar device
 to control the helicopter, and getting it to hover in place.
 He goes no further than this, and leaves the idea as an open problem to
 the reader.
\end_layout

\begin_layout Paragraph
Ground work
\end_layout

\begin_layout Standard
With the idea as a starting point, initial research was carried out on where
 to begin.
 The first place to start was with the communication method between the
 computer and the helicopter - without this, the whole project could not
 progress any further.
 Additionally, the communication method needed to be robust and reliable,
 so as to not make the remaining tasks any more difficult than they needed
 to be.
\begin_inset Newline newline
\end_inset

In his blog post, Lee discusses using a Teensy USB developement board 
\begin_inset CommandInset citation
LatexCommand cite
key "teensyusb"

\end_inset

.
 However, Teensy USB boards are only made in the US, so other alternatives
 were considered.
 The foremost of these was the Arduino board, a popular microcontroller
 available globally, and one was shortly obtained.
\begin_inset Newline newline
\end_inset

With the help of some research on building circuits, using IR LEDs and using
 the Arduino itself 
\begin_inset CommandInset citation
LatexCommand cite
key "Arduino,Arduino port,IR"

\end_inset

, the final working circuit was made.
 This is discussed in more detail later on.
\begin_inset Newline newline
\end_inset

With this completed, we carried out research on similar projects and ideas
 that could provide benefit to our system.
\end_layout

\begin_layout Section
Prior work
\end_layout

\begin_layout Standard
Similar examples of automated flying have been attempted many times, generally
 using quadcopters instead of helicopters.
 Quadcopters (also known as quadrocopters) are small flying machines that
 have four individual propellers arranged in a square.
 Navigation is accomplished by powering the propellers at different speeds,
 causing the platform as a whole to tilt.
 Quadcopters are reknown for their stability and simple design, however
 they are often more expensive as the control method is more complicated,
 and they have more moving parts than our RC helicopter.
 Nevertheless, these examples provided inspiration for many different parts
 of the finished system.
\begin_inset Newline newline
\end_inset

These prior works are not as useful in the traditional sense of background
 material, that is, they do not neccessarily provide a foundation from which
 we can work from.
 To the contrary, they more serve as examples of similar problems and how
 they have been solved.
\end_layout

\begin_layout Subsection
Quadrocopter experiments, Technical University Munich 
\begin_inset CommandInset citation
LatexCommand cite
key "Camera based nav of low cost quadropter"

\end_inset


\end_layout

\begin_layout Standard
Three students at the Technical University Munich wrote a paper titled 
\begin_inset Quotes eld
\end_inset

Camera-Based Navigation of a Low-Cost Quadrocopter
\begin_inset Quotes erd
\end_inset

, detailing 
\begin_inset Quotes eld
\end_inset

a system that enables a low-cost quadrocopter coupled with a ground-based
 laptop to navigate autonomously in previously unknown and GPS-denied environmen
ts
\begin_inset Quotes erd
\end_inset

.
 Many parallels can be made with this project and ours - it is performed
 using a low-cost flying device, without customised, high-precision location-dat
a sources, uses (amongst other things) a monocular vision device, and works
 indoors in previously unknown environments.
 However, it is also varies in others, not just in the fact that it is using
 a quadcopter instead of a helicopter - it also has access to other on-board
 camera sources and flight information (having a 3-axis gyroscope, accelerometer
 and an ultrasound altimeter).
 
\end_layout

\begin_layout Paragraph
PID Control
\end_layout

\begin_layout Standard
One key method that they use for control in this project is the PID (proportiona
l-integral-derivative) controller.
 PID control was used to steer the quadrocopter towards a location.
 They used a separate PID controller for each degree of freedom of the quadrocop
ter, and experimentally calibrated each controller.
 We can do something similar in our project - with only two major degrees
 of freedom (the yaw is controlled by a gyroscope, so it is very predictable
 in nature), it should also be substantially simpler.
 We will look into PID control in more detail later on.
\end_layout

\begin_layout Paragraph*
Results
\end_layout

\begin_layout Standard
In the quadropter experiments, they successfully demonstrated that accurate
 and robust visual navigation is feasible using a simple, low-cost hardware,
 in a variety of environments.
 Using a combination of machine learning and visual systems from a monocular
 source, combined with onboard data, they managed to achieve an average
 positioning accuracy of 4.9cm whilst flying indoors.
 This is quite an encouraging result - whilst we will not have access to
 onboard flying information, our positional accuracy does not need to be
 as precise.
 
\end_layout

\begin_layout Subsection
Autonomous Camera-Based Navigation of a Quadrocopter
\begin_inset CommandInset citation
LatexCommand cite
key "Autonomous Camera based quadropter"

\end_inset


\end_layout

\begin_layout Standard
In his master's thesis, Jacob Engel discusses a system for enabling a quadrocopt
er to 
\begin_inset Quotes eld
\end_inset

localize and navigate autonomously in previously unknown and GPS-denied
 environments...
 using a monocular camera onboard the quadrocopter
\begin_inset Quotes erd
\end_inset

.
 Much like the paper discussed above, he uses a SLAM system for positional
 estimates, combines this with an Extended Kalman Filter to fuse all available
 data and to compensate for data delays, and a PID controller to then pefrom
 actual control.
 
\end_layout

\begin_layout Paragraph*
Monocular SLAM
\end_layout

\begin_layout Standard
SLAM (simultaneous localization and mapping) is a computer vision technique
 used to build up a map of an environment containing unknown factors, whilst
 at the same time keeping track of current location.
 Many different types of sensors can be used to solve SLAM-based problems,
 such as laser range scanners, monocular or stereo cameras, or RGB-D sensors
 (RGB camera plus a depth sensor eg.
 Kinect).
 In the quadropter experiments, it is used to keep track of the quadropter
 relative to the environment.
 These SLAM systems are generally located on board the vehicle itself.
 
\begin_inset Newline newline
\end_inset

SLAM will most likely not end up being a part of our system.
 Instead, we will focus on getting the automated flight correct in a assumed-emp
ty flying space.
 However, it does pose an interesting idea for later developement - perhaps
 the creation of a map of the surrounding environment, to be navigated by
 the helicopter?
\end_layout

\begin_layout Subsection
KinectFusion: Real-time 3D Reconstruction and Interaction Using a Moving
 Depth Camera
\begin_inset CommandInset citation
LatexCommand cite
key "Kinectfusion"

\end_inset


\end_layout

\begin_layout Standard
Whilst the fundamental topics of this paper are somewhat different than
 our project, it still covers many different areas of potential extension.
 The KinectFusion is a method for creating a 3D representation of a scene
 by using a Kinect, that uses depth values from multiple angles to create
 the model, in real-time.
 It might be possible to utilize some of the techniques used to create a
 partial construction of the environment by using a single location from
 the Kinect, which can then later be used during flight.
\end_layout

\begin_layout Paragraph
KinectFusion
\end_layout

\begin_layout Standard
The Kinect, whilst being a great tool for collecting depth data as well
 as RGB data, is still subject to small errors in its depth calculations.
 This means that when we want to recreate the environment as a point map
 when using the depth data, we have to be aware that the resulting model
 is not necessarily accurate.
 The interactive reconstruction system that is KinectFusion aims to solve
 these, by combining different viewpoints of the scene into on single 3D
 representation.
 The Kinect is moved around the scene as a handheld scanner, recreating
 the model in realtime.
 
\end_layout

\begin_layout Paragraph
Remarks
\end_layout

\begin_layout Standard
The KinectFusion offers many interesting thinking points.
 Partial scene reconstruction would certainly be useful for the later stages
 of this project, where it is important to be aware of the surroundings
 relative to the location of the helicopter.
 For instance, it could be used to create a 'safe' flying environment, where
 the system allows free flight for the user, but steps in if a collision
 might occur.
 This 'assisted flying' mechanism could especially be useful for beginner
 pilots!
\end_layout

\begin_layout Subsection
Computer Controlling a Syma Helicopter
\begin_inset CommandInset citation
LatexCommand cite
key "Syma"

\end_inset


\end_layout

\begin_layout Standard
This blog post by Johnny Chung Lee was the original inspiration for this
 project, and lists a potential implementation method for the manual control
 of the helicopter from a computer.
 Additionally, it lists some rough measurements for the IR protocol, and
 it was from these measurements that we based our initial control parameters
 on.
 After using an oscilloscope to measure the signal sent from the radio controlle
r, he came up with the following approximate values:
\end_layout

\begin_layout Paragraph
IR Protocol
\end_layout

\begin_layout Itemize
The signal is modulated at 38KHz
\end_layout

\begin_layout Itemize
The packet header is 2ms on, the 2ms off.
\end_layout

\begin_layout Itemize
The total packet payload is 4 bytes, in big-endian order.
\end_layout

\begin_layout Standard
Then the individual command values are:
\end_layout

\begin_layout Itemize
Yaw, which takes values 0 - 127, 0 being full reverse yaw and 127 being
 full forward yaw.
 63 is no yaw at all.
\end_layout

\begin_layout Itemize
Pitch, which takes values 0 - 127, 0 being full reverse pitch and 127 being
 full forward pitch.
 63 is no pitch at all.
\end_layout

\begin_layout Itemize
Throttle, which takes values 0 - 127, 0 being no throttle and 127 being
 full throttle.
\end_layout

\begin_layout Itemize
Yaw Correction, which takes values 0 - 127, 63 being no correction.
\end_layout

\begin_layout Standard
The packet is completed with a stop bit ('1').
 The individual bit values are formatted as '1' being 
\begin_inset Formula $320\mu s$
\end_inset

 on, then 
\begin_inset Formula $680\mu s$
\end_inset

 off, and '0' being 
\begin_inset Formula $320\mu s$
\end_inset

 on, then 
\begin_inset Formula $280\mu s$
\end_inset

 off.
 These packets should be sent to the helicopter every 120ms.
 
\begin_inset Newline newline
\end_inset

However, he goes on to report that his personal experiments with these values
 results in stuttery performance.
\end_layout

\begin_layout Paragraph
Personal findings
\end_layout

\begin_layout Standard
We implemented the above protocol myself using the Arduino board, and found
 that the helicopter did not respond in an optimal fashion, just as Lee
 previously discussed in the blog post.
 The helicopter would operate as ordered for a few packets, and then stop
 for a small amount of time, and then restart again.
 To remedy this, we performed some experiments with the timings, and found
 that the following timings provided continuous performance with no stuttering
 or lost signals:
\end_layout

\begin_layout Itemize
Same values for '1' and '0' pulses, along with message length and the header.
\end_layout

\begin_layout Itemize
Footer of length 
\begin_inset Formula $300\mu s$
\end_inset

 .
\end_layout

\begin_layout Itemize
A delay of 
\begin_inset Formula $25\mu s$
\end_inset

 between each message sent.
\end_layout

\begin_layout Itemize
Compensating time for the individual port switches (basically compensating
 for communication time).
\end_layout

\begin_layout Standard
After duplicating this signal across multiple IR LEDs, this also allowed
 for long-range control, well outside of the normal operating range for
 the helicopter in an room indoors.
\end_layout

\begin_layout Paragraph
Basic helicopter mechanics
\end_layout

\begin_layout Standard
The helicopter itself consists of three different rotors (two on the main
 rotor, and one on the tail), and a gyroscope.
 The two sets of blades on the main rotor rotate in different directions,
 the top blades rotating clockwise and the lower ones rotation counter-clockwise.
 This has the effect of cancelling out their torque when they move at the
 same speed, keeping the helicopter from rotating in one particular direction.
 The ratio that these blades move to each other can be altered in small
 amounts for fine tuning.
 As discussed above, the rear rotor is positioned in an upright position,
 just like the main rotors.
\end_layout

\begin_layout Itemize
Vertical acceleration is gained by increasing the throttle to the two main
 rotors in equal amounts.
 
\end_layout

\begin_layout Itemize
Yaw rotation is gained by increasing one of the main rotors, and decreasing
 the other an equivalent amount.
 This will give an overall clockwise or counter-clockwise torque without
 affecting the vertical acceleration produced.
 
\end_layout

\begin_layout Itemize
Pitch movement is gained by spinning the tail rotor.
 Reversing the spin of the tail rotor results in opposite pitch.
 The gyroscope on top of the main helicopter rotor prevents the helicopter
 from pitching forwards too much by providing a counter force.
\end_layout

\begin_layout Section
Control methods
\end_layout

\begin_layout Standard
Once relevant data about the helicopter has been obtained, it is necessary
 to actual control the helicopter in 3D space.
 There are many different approaches to this, each with their own benefits
 and drawbacks.
 For instance, for the Throttle of the helicopter it will almost certainly
 be necessary to have some sort of adaptive control.
 This will be to combat the effect of reduced effective throttle as the
 battery runs out in the helicopter (eg.
 at full-charge, the helicopter will remain level at a Throttle of about
 70.
 As the charge reduces below half, it will require upwards of 90-100 to
 remain level).
 In contrast, Yaw does not need any kind of adaptive control as it is controlled
 on board the helicopter by a gyroscope, and can instead be operated in
 a digital fashion.
 We will look at some of these control methods here.
\end_layout

\begin_layout Subsection
PID Controller
\end_layout

\begin_layout Standard
The majority of the papers mentioned above used a PID Controller (Proportional
 Integral Derivative Controller) as a control mechanism.
 The PID controller is a simple and robust method of controlling a device
 by performing control loop feedback.
\end_layout

\begin_layout Paragraph
Formulation
\end_layout

\begin_layout Standard
The PID mechanism is best described by its three constituent parts; a proportion
al term, integral term, and a derivative term.
 The sum of these three terms makes the new input parameter.
 That is,
\begin_inset Formula 
\[
input=proportional+integral+derivative
\]

\end_inset

Each of the three terms is comprised of an input parameter (
\begin_inset Formula $K_{p},K_{i}$
\end_inset

and 
\begin_inset Formula $K_{d}$
\end_inset

 for proportional, integral and derivative respectively), and some combination
 of the previous and current error in the sytem (the error being the difference
 between the current state and the desired state).
 These are described in more detail below.
\end_layout

\begin_layout Paragraph
Proportional term
\end_layout

\begin_layout Standard
The proportional term is defined as 
\begin_inset Formula 
\[
K_{p}e(t)
\]

\end_inset

where 
\begin_inset Formula $e(t)$
\end_inset

 is the error at time 
\begin_inset Formula $t$
\end_inset

.
 As described by its name, this term increases proportional to the size
 of the error between the desired and current state.
 As such, large 
\begin_inset Formula $K_{p}$
\end_inset

 values will cause a large change in the input value as the error change,
 which can cause system instability (see later for more on stability).
\end_layout

\begin_layout Paragraph
Integral term
\end_layout

\begin_layout Standard
The integral term is defined as
\begin_inset Formula 
\[
K_{i}\int_{0}^{t}e(x)dx
\]

\end_inset

where 
\begin_inset Formula $\int_{0}^{t}e(x)dx$
\end_inset

 is the cumulative error over time.
 This essentially accounts for error that should have previously been corrected
 by the system, and grows as this error perpetuates.
\end_layout

\begin_layout Paragraph
Derivative term
\end_layout

\begin_layout Standard
The derivative term is defined as
\begin_inset Formula 
\[
K_{d}\frac{d}{dt}e(t)
\]

\end_inset

or in a discrete system, simply
\begin_inset Formula 
\[
K_{d}(e(t)-e(t-1))
\]

\end_inset

which is the rate of change of the error.
 This accounts for the 
\begin_inset Quotes eld
\end_inset

approach speed
\begin_inset Quotes erd
\end_inset

 of the device, predicting the 
\begin_inset Quotes eld
\end_inset

velocity
\begin_inset Quotes erd
\end_inset

 of the error and dampening it accordingly.
\end_layout

\begin_layout Paragraph
Tuning
\end_layout

\begin_layout Standard
Each of the parameters reflects a different property of the control loop
 that needs to be addressed.
 The proportional parameter reflects the response to error - small values
 will not have a significant effect on the input, however large errors can
 cause unwanted oscillatory behaviour.
 The integral parameter deal with accumulated error - it is in this way
 that it can compensate for 'droop' in the system, where there is a control
 bias that needs to be accounted for.
 Small values will take a long time to compensate for these, but large values
 will overcompensate causing overshooting.
 The derivative term 'dampens' the approach, but will increase the time
 required to reach stability.
 
\begin_inset Newline newline
\end_inset

Each of the costs and benefits of these terms needs to be weighted against
 the other, and this process is referred to as 'tuning' the controller.
 There are a variety of methods for tuning the controller, from simple manual
 observation and adjustment to full automated methods.
\end_layout

\begin_layout Paragraph
Suitability
\end_layout

\begin_layout Standard
The PID controller was used in the majority of the quadcopter papers, so
 it definitely has a use in automated flying.
 For the helicopter in particular, it fits the requirements very well -
 in the vertical case, for instance, the input is the throttle and the output
 is the observed position, with the error being the difference between the
 required position and the current position.
 We require this more complex controller because we do not know the given
 'neutral' Throttle required to remain stationary against the effect of
 gravity.
 In comparison, this is not true for our Yaw or Pitch controls, which we
 know give a neutral position with the value 63.
\end_layout

\begin_layout Subsection
Linear Control
\end_layout

\begin_layout Standard
As the only opposing force to the motion of the helicopter is air resistance
 itself, plus the helicopter's own rotor motion, it is desirable to have
 some sort of pre-emptive slow down mechanism so as to not 'overshoot' the
 intended position.
 In the lecture notes Steering Behaviours
\begin_inset CommandInset citation
LatexCommand cite
key "ludic"

\end_inset

, the 'Arrive' steering behaviour is discussed - it linearly decreases the
 desired speed based on distance between the desired position and the current
 position.
 This decrease in speed can be achieved by simple air-resistance drifting
 (requires a longer distance) or by applying a counter force (shorter distance).
\begin_inset Newline newline
\end_inset

If 
\begin_inset Formula $p$
\end_inset

 is the current position, and 
\begin_inset Formula $q$
\end_inset

 is the desired position, then
\begin_inset Formula 
\[
d_{off}=Target\, distance=|q-p|
\]

\end_inset

and
\begin_inset Formula 
\[
ideal\, speed\,\sigma=\begin{cases}
s_{max} & d_{off}>d_{stop}\\
s_{max}\cdot\frac{d_{off}}{d_{stop}} & otherwise
\end{cases}
\]

\end_inset

Then, with desired velocity 
\begin_inset Formula $u=\sigma\cdot unit(q-p)$
\end_inset

, the final desired change in velocity (taking into account the current
 velocity) is
\begin_inset Formula 
\[
Arrive(q)=u-v
\]

\end_inset


\end_layout

\begin_layout Paragraph
Suitability
\end_layout

\begin_layout Standard
It is already known that a neutral Pitch (Pitch = 63), ignoring air currents
 in the room, will not move the helicopter forwards or backwards, so as
 long as the forward Pitch is cut off sufficiently early.
 Additionally we are capable of providing reverse thrust as required simply
 by altering the Pitch in the opposite direction.
 As such, the 'Arrive' behaviour is suitable for movement in the 
\begin_inset Formula $x-z$
\end_inset

 plane.
\end_layout

\begin_layout Subsection
Digital Control
\end_layout

\begin_layout Standard
When the movement mechanism is known to be 'digital' in nature, in that
 it is possible to start and stop movment at a single moment, it should
 be sufficient to have an extremely simple 'digital' control mechanism as
 well.
 In pseudo-code, we could represent this as
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

currentDirection = observe
\end_layout

\begin_layout Plain Layout

diff = currentDirection - desiredDirection
\end_layout

\begin_layout Plain Layout

if diff > 0:
\end_layout

\begin_layout Plain Layout

	setMotor LEFT
\end_layout

\begin_layout Plain Layout

elif diff < 0:
\end_layout

\begin_layout Plain Layout

	setMotor RIGHT
\end_layout

\begin_layout Plain Layout

else:
\end_layout

\begin_layout Plain Layout

	setMotor STOP
\end_layout

\end_inset

Depending on the situation or accuracy required, we could alter the speed
 of the movement, whilst still retaining the ability to stop at a whim.
\end_layout

\begin_layout Paragraph
Suitability
\end_layout

\begin_layout Standard
For our helicopter system, this kind of digital control is most appropriate
 for the Yaw of the helicopter.
 Setting the Yaw value to 63 will immediately halt any clockwise/anticlockwise
 motion.
 Subsequently, if we wish to reorient the helicopter, it is sufficient to
 apply some steady left/right Yaw (eg.
 43 or 83 for left/right respectively), wait for the orientation to be as
 desired, then apply Yaw = 63 again.
\end_layout

\begin_layout Section
Vision
\end_layout

\begin_layout Standard
As has been mentioned earlier, we will use a Kinect camera as our vision
 input source.
 The Kinect is not just an RGB camera - it is also a depth sensor, and this
 extra dimension of input could potentially simplify many of the problems
 that we are solving.
 
\end_layout

\begin_layout Subsection
Kinect 
\end_layout

\begin_layout Itemize
Calibration
\end_layout

\begin_layout Itemize
Camera parameters
\end_layout

\begin_layout Itemize
Converting from depth to RGB camera (using params)
\end_layout

\begin_layout Subsection
OpenCV
\end_layout

\begin_layout Standard
For my initial research on OpenCV, I began reading some tutorials and online
 documentation
\begin_inset CommandInset citation
LatexCommand cite
key "OpenCV book"

\end_inset

, as well as looking at existing examples in the Haskell wrapper for OpenCV
\begin_inset CommandInset citation
LatexCommand cite
key "OpenCV examples"

\end_inset

.
 I aimed to follow the general learning flow from the book (implemented
 in C++), but to do it myself in Haskell, thus hopefully learning both OpenCV
 and the wrapper itself.
 My first aim after doing some basic exercises was to get basic shape recognitio
n working on a plain white background (eg.
 my wall).
 
\end_layout

\begin_layout Standard
Unfortunately, after spending some time trying to get the Haskell wrappers
 to work, I came to the conclusion that the effort required to get the libraries
 up and running in Haskell was not worth the gain.
 Instead, I decided to use one of the better supported languages for OpenCV
 - Python or C++.
 Both of these have very thorough support for the OpenCV library, letting
 me avoid the problem of language-wrestling entirely.
 Python in particular is incredibly simple, not just in language terms but
 in library support - I discovered the open source framework SimpleCV (http://si
mplecv.org/), which is a large collection of libraries written in Python
 with the aim of providing simplicity.
 This should come in especially useful during the early prototyping phases.
 For instance, getting the Kinect RGB image is as simple as:
\begin_inset listings
lstparams "breaklines=true,language=Python,numbers=left,showstringspaces=false,tabsize=2"
inline false
status open

\begin_layout Plain Layout

import SimpleCV
\end_layout

\begin_layout Plain Layout

cam = SimpleCV.Kinect()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

while True:
\end_layout

\begin_layout Plain Layout

	cam.getImage().show()
\end_layout

\end_inset

Due to the modularity of the vision part of the project, it should be comparitiv
ely simple to add a foreign function call to the Haskell code, and then
 implement the whole vision package in a different language.
 
\end_layout

\begin_layout Standard
To make things even simpler, SimpleCV is also documented very well, with
 all of the documentation available online and inside an interpreter.
 Having access to an interpreter makes experimentation very easy, especially
 considering the fact that Python has first-class functions, letting me
 manipulate images and test out new image processing techniques into existing
 code very easily.
 In addition to the code documentation, I also have started reading the
 book Practical Computer Vision with SimpleCV 
\begin_inset CommandInset citation
LatexCommand cite
key "SimpleCV book"

\end_inset

, which lists many different examples and techniques to get me started.
\end_layout

\begin_layout Subsection
SimpleCV
\end_layout

\begin_layout Itemize
Connection with OpenCV (freenect)
\end_layout

\begin_layout Itemize
Additional features
\end_layout

\begin_layout Itemize
Easily expanded/altered
\end_layout

\begin_layout Section
Language tools
\end_layout

\begin_layout Standard
The language Haskell was used for the bulk of the coding because of a variety
 of tools that it brings to the project.
 These include simple concurrency through the use of STMs, a robust package
 management system and a powerful interpreter for testing individual modules.
 In this section we will go over each of these different areas and analyse
 what they offer to this project.
\end_layout

\begin_layout Paragraph
Standard Transactional Memory
\end_layout

\begin_layout Standard
Haskell comes packaged with a powerful method of concurrency, under the
 guise of the STM monad.
 The STM monad allows...
 
\begin_inset Newline newline
\end_inset

STM works by...
\begin_inset Newline newline
\end_inset

STM is more useful in this case because...
\end_layout

\begin_layout Paragraph
Cabal package management
\end_layout

\begin_layout Standard
Whilst creating a large project, it is unavoidable that many other modules
 will be imported along the way (unless one wishes to write everything themselve
s from scratch).
 The default package manager for Haskell, Cabal, manages all of this...
\begin_inset Newline newline
\end_inset

Cabal was picked instead of other package managers such as...
\end_layout

\begin_layout Paragraph
GHCi 
\end_layout

\begin_layout Standard
GHCi is the interpreter built into the Glasgow Haskell Compiler.
 It allows the user to dynamically load modules into the interpreter, automatica
lly locating dependencies and loading those too.
 The user can then use any of these loaded functions in the interpreter
 environment, creating data structures dynamically and testing the system.
 
\begin_inset Newline newline
\end_inset

In this project it was particularly useful for testing out various data
 structures and learning methods.
 For example, it was useful to be able to create a PID controller inside
 the interpreter environment, and then provide it with dummy values to see
 how the corresponding output would change with input.
 Whilst being beneficial for general prototyping, it was also useful for
 learning!
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Chapter
System Architecture
\end_layout

\begin_layout Standard
The helicopter control system is built from several different modules that
 interact with each other in a set manner.
 The modular structure is beneficial as it allows easy separation of responsibil
ities within the system, and also allows 'swapping' of certain modules with
 similar roles but different methods.
 The majority of the vision system is written in Python, the Arduino in
 (obviously) Arduino, and the control system in Haskell.
 Additionally, some of the debugging and simulation programs are written
 in Python.
\end_layout

\begin_layout Section
Crossing points
\end_layout

\begin_layout Standard
There are few key crossing points between different modules, where the different
 languages must cooperate with each other.
 These can roughly be seen in the diagram below:
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename Layout.png
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Simulations and control are all centralised to the Haskell 
\begin_inset Quotes eld
\end_inset

Majom
\begin_inset Quotes erd
\end_inset

 module.
 Control data is sent to the Helicopter via an Arduino device, which is
 communicated to through a serial port.
 Positional data is queried from the Vision module via a simple local HTTP
 server.
 Lastly, positional data from the simulation is sent straight to 
\series bold

\begin_inset Formula $stdout$
\end_inset


\series default
, which can be picked up directly by the Python simulation viewer.
\end_layout

\begin_layout Section
Modules
\end_layout

\begin_layout Standard
Each module has a defined set of tasks and responsibilities that it handles.
 These are briefly outlined below.
\end_layout

\begin_layout Subsubsection*
Vision
\end_layout

\begin_layout Standard
The vision system is written entirely in Python.
 Python was chosen for its extensive libraries, high level nature and ease
 of prototyping with its interpreter.
 Using the SimpleCV library, the module connects to the Kinect camera, and
 then combined with some computer vision techniques it obtains 3D positional
 and directional data of the Helicopter.
 This data is served up via a local HTTP server as needed by the control
 module.
\end_layout

\begin_layout Subsubsection*
Control
\end_layout

\begin_layout Standard
The control system is built in Haskell.
 The system uses a Flyer Type Class, which fully defines all of the required
 traits of something that is 
\begin_inset Quotes eld
\end_inset

flyable
\begin_inset Quotes erd
\end_inset

.
 Haskell was used for several reasons - it has a good amount of libraries,
 is strongly typed (useful for testing) and is (largely) pure.
 Additionally, I (the project writer) have personal experience developing
 AI in Haskell from previous projects.
\end_layout

\begin_layout Paragraph*
Flyable
\end_layout

\begin_layout Standard
A Flyable is a Type Class that must be able to receive orders, have an active/in
active status, and most importantly must be observeable.
 Concrete Types can become instances of the Flyable, such as the actual
 Helicopter itself (with observations coming from the Vision module), or
 a VirtualHelicopter (with observations coming directly from the simulator
 itself).
 Other, more complicated Flyables can also be defined, such as the DuoCopter
 which acts as an intermediary that allows two Controllers to fly one helicopter
 at once!
\end_layout

\begin_layout Paragraph*
Controller
\end_layout

\begin_layout Standard
Several different controllers have been defined, each interacting with the
 Helicopter in different ways.
 For instance, the GUI controller provides a method for the user to fly
 the Helicopter using the keyboard.
\begin_inset Newline newline
\end_inset

The main Controller of interest is the Monkey (the namesake of the project
 itself).
 The Monkey aims to learn the various variables that define how the Helicopter
 flies through reinforcement techniques, much like a human with some basic
 understanding of how a Helicopter flies would.
\end_layout

\begin_layout Subsubsection*
Arduino
\end_layout

\begin_layout Standard
The Arduino board receives information on what 'orders' to send to the helicopte
r via a serial cable.
 It works in cycles, receiving all new orders, then consolidating them into
 a final order set before sending this to the helicopter.
 It is written in the Arduino language, a derivative of Processing.
 It is a high level language that obscures a lot of the lower level working
 of the board itself, and come pre-packaged with the Arduino board itself.
\end_layout

\begin_layout Chapter
Execution
\end_layout

\begin_layout Standard
One of the key aims for the development of this project was to be able to
 remain flexible, and to quickly prototype new ideas and methods to see
 which one worked best for a given task.
 To achieve this, a solid 'framework' needed to be planned, assigning specific
 roles to specific areas, and ensuring that the abstract version would in
 fact work.
 
\end_layout

\begin_layout Section
Planning
\end_layout

\begin_layout Standard
To be able to properly develop in a flexible manner, it was important to
 lay out a good foundation to develop upon.
 First, the project was divided into distinct milestones, which should in
 theory be achieved in order, with each milestone providing a place to fall
 back on if a particular method does not work out.
 In the initial iteration, the milestones were
\end_layout

\begin_layout Itemize
Fly the helicopter manually from the computer
\end_layout

\begin_layout Itemize
Set up a basic simulation environment for the helicopter
\end_layout

\begin_layout Itemize
Track the helicopter in a simple fashion in a simplified environment
\end_layout

\begin_layout Itemize
Develop a basic controller that can fly the simulated controller
\end_layout

\begin_layout Itemize
Combine controller and simple vision to have a simple flyer
\end_layout

\begin_layout Itemize
Improve upon vision system to make it more robust
\end_layout

\begin_layout Itemize
Improve upon control system to allow for more complex instructions
\end_layout

\begin_layout Standard
As would be revealed as the project progressed, not all of these proved
 to be viable milestones.
\begin_inset Newline newline
\end_inset

In addition to having some set milestones, it was important to have a good
 skeletal structure for each module, especially so for the controller.
 This involved setting up contracts in the form of Type Classes for different
 parts of the controller, that each new test module would need to fulfill.
 This helped to realise the requirements for each module, and made testing
 much easier.
 For instance, the Flyable Type Class, which describes a 
\begin_inset Quotes eld
\end_inset

Flyable
\begin_inset Quotes erd
\end_inset

 object is written as:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Haskell,numbers=left,tabsize=2"
inline false
status open

\begin_layout Plain Layout

-- | Flyable class for flyable things that can fly.
\end_layout

\begin_layout Plain Layout

class Flyable a where
\end_layout

\begin_layout Plain Layout

  setFly :: a -> Option -> Int -> IO ()
\end_layout

\begin_layout Plain Layout

  setFlyMany :: a -> [(Option, Int)] -> IO ()
\end_layout

\begin_layout Plain Layout

  fly :: a -> IO ()
\end_layout

\begin_layout Plain Layout

  observe :: a -> IO (Power, Position)
\end_layout

\begin_layout Plain Layout

  isActive :: a -> IO Bool
\end_layout

\begin_layout Plain Layout

  setActive :: a -> Bool -> IO ()
\end_layout

\end_inset

Each function describes a responsibility for a Flyable object, that must
 be fulfilled.
 These include the key components of flying a helicopter, such as sending
 commands (
\begin_inset listings
lstparams "language=Haskell,tabsize=2"
inline true
status open

\begin_layout Plain Layout

setFly
\end_layout

\end_inset

) and observing the whereabouts of the helicopter (
\begin_inset listings
lstparams "language=Haskell,tabsize=2"
inline true
status open

\begin_layout Plain Layout

observe
\end_layout

\end_inset

), as well as more minor things such as classifying if a flyable is actually
 active.
 
\end_layout

\begin_layout Subsection
Arduino / Helicopter communication
\end_layout

\begin_layout Standard
One of the very beginning parts of the project was to acquire and set up
 an Arduino system for controlling the helicopter.
 As the whole project itself hinges on the helicopter being flown from the
 computer, this was an important foundation step.
 With some helpful tutorials
\begin_inset CommandInset citation
LatexCommand cite
key "Syma,Syma2"

\end_inset

, and online Arduino documentation
\begin_inset CommandInset citation
LatexCommand cite
key "Arduino,Arduino port"

\end_inset

 it was possible to get a first iteration of the helicopter control up and
 running from a laptop, and from there it was a simple case of trial and
 error to get a reliable connection to the helicopter.
 In the final iteration, the Arduino board waits on a serial connection
 from the computer, reads the incoming commands and then translates them
 into commands to relay to the helicopter.
 No response is expected from the helicopter itself.
\end_layout

\begin_layout Paragraph
Arduino Set Up
\end_layout

\begin_layout Standard
The circuit layout of the Arduino is quite simple - at its most basic level
 it provides a resistor and infra-red LED attached to the digital out pin.
 This is replicated several times to increase the range at which the helicopter
 can operate as well as reducing the likelihood of the signal being obscured.
 The circuit layout can be seen in Figure 4.1.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename arduino_circuit.png
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
Circuit diagram of the circuit used to communicate with the helicopter.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename arduino_real.JPG
	lyxscale 10
	width 8cm

\end_inset


\end_layout

\end_inset

Photo of the final circuit and Arduino board.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Helicopter communication specifics
\end_layout

\begin_layout Description
Transmission
\begin_inset space ~
\end_inset

Method The Arduino communicates with the helicopter through an IR LED installed
 on the Arduino.
 The LED is pulsed on and off to represent the signal 
\begin_inset Quotes eld
\end_inset

on
\begin_inset Quotes erd
\end_inset

, which combined with periods of no signal give binary 0 and 1.
 A 
\begin_inset Quotes eld
\end_inset

1
\begin_inset Quotes erd
\end_inset

 bit is 320 microseconds on, 680 microseconds off, and a 
\begin_inset Quotes eld
\end_inset

0
\begin_inset Quotes erd
\end_inset

 bit is 320 microseconds on, 280 microseconds off.
\end_layout

\begin_layout Description
Packet
\begin_inset space ~
\end_inset

Measurements A packet consists of a header, the main order packet, and a
 footer.
 A header consists of 2000 microseconds on, then 2000 microseconds off.
 A footer consists of 300 microseconds on.
\begin_inset Newline newline
\end_inset

The main packet itself is 32 individual bits.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The first 8 bits (0-8) represent the Yaw value, with 63 being neutral, 0
 being full left yaw and 127 being full right yaw.
 
\end_layout

\begin_layout Itemize
The second 8 bits (8-16) represent the Pitch.
 Like Yaw, 63 is neutral, 0 is full back pitch and 127 is full forwards
 pitch.
\end_layout

\begin_layout Itemize
The third 8 bits (16-24) are the Throttle.
 This ranges from 0 (no throttle) to 127 (full throttle).
\end_layout

\begin_layout Itemize
The last 8 bits (24-32) are the Correction.
 Correction is a small amount of offset that can be specified manually to
 keep the helicopter from turning when the controller is in a neutral setting.
\end_layout

\end_deeper
\begin_layout Itemize
The structure of the packet can be easily seen from the Arduino code itself
 -
\begin_inset listings
lstparams "numbers=left,tabsize=2"
inline false
status open

\begin_layout Plain Layout

void SendCode() {
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  pulseIR(headerOn);
\end_layout

\begin_layout Plain Layout

  delayMicroseconds(headerOff);
\end_layout

\begin_layout Plain Layout

  pulseLength= headerOn + headerOff;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  for(int i=0; i < 32; i++) {
\end_layout

\begin_layout Plain Layout

    sendPulseValue(pulseValues[i]);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  //Footer
\end_layout

\begin_layout Plain Layout

  pulseIR(footerOn);
\end_layout

\begin_layout Plain Layout

  delayMicroseconds(messageLength - pulseLength - footerOn); 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Description
Communication
\begin_inset space ~
\end_inset

Loop A typical communication loop with Arduino involves:
\end_layout

\begin_deeper
\begin_layout Enumerate
Wait for incoming instructions from the serial port.
\end_layout

\begin_layout Enumerate
Filter the incoming instruction on instruction type (Yaw, Pitch, Throttle
 or Correction).
 Set the relevant bits in the new order packet based on these.
\end_layout

\begin_layout Enumerate
If there are no more incoming instructions, send the code using the protocol
 specified above.
 Otherwise, process new instructions and edit order packet accordingly.
\end_layout

\begin_layout Enumerate
Delay by 25 microseconds, then loop.
 This 25 microsecond delay prevented too many orders being sent to the helicopte
r in quick succession, thus confusing it.
\end_layout

\end_deeper
\begin_layout Section
Framework
\end_layout

\begin_layout Standard
An important part of the initial set up was developing suitable tools to
 make testing and development much easier.
 These could be best simplified to two parts - reliable communication methods
 between different modules, and powerful test tools to make prototyping
 and developing much easier.
\end_layout

\begin_layout Subsection
Communication methods
\end_layout

\begin_layout Standard
Some considerable thought was put into deciding what method should be used
 to link different language sections of the system.
 With such vastly different languages being used as Python and Haskell,
 it was no simple thing to just 
\begin_inset listings
lstparams "language=Python"
inline true
status open

\begin_layout Plain Layout

import monkey
\end_layout

\end_inset

 or 
\begin_inset listings
lstparams "language=Haskell"
inline true
status open

\begin_layout Plain Layout

import Vision
\end_layout

\end_inset

!
\begin_inset Newline newline
\end_inset

The main requirements for the communication method were simple; it needed
 to 
\end_layout

\begin_layout Itemize
Support the sending of a tuple/list of doubles, somewhere around the range
 of 4 or 5.
 This is to transmit the 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $y$
\end_inset

, 
\begin_inset Formula $z$
\end_inset

 and postional information from the vision system to the control system.
\end_layout

\begin_layout Itemize
Be fast.
\end_layout

\begin_layout Standard
Initially, we were hoping that it would be possible to use the Foreign Function
 Interface (FFI) 
\begin_inset CommandInset citation
LatexCommand cite
key "FFI"

\end_inset

 feature that comes with Haskell.
 However, it proved much more complicated to use than previously hoped,
 and it became evident early on that it would be wiser to search for alternative
 methods.
\end_layout

\begin_layout Subsubsection
Thrift
\end_layout

\begin_layout Standard
One method that was discovered was the software framework Apache Thrift
 
\begin_inset CommandInset citation
LatexCommand cite
key "Thrift"

\end_inset

.
 This seemed like a simple, effective and most of all flexible solution
 to the communication problem.
 With Thrift, it was possible to design C-like language-independent data
 structures, and then Thrift would provide all of the heavy lifting and
 translation between the two languages as required.
 For instance, for the simple relaying of positional information, the data
 structure definition was simply
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

struct Position {
\end_layout

\begin_layout Plain Layout

  1: double x,
\end_layout

\begin_layout Plain Layout

  2: double y,
\end_layout

\begin_layout Plain Layout

  3: double z
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

service Vision {
\end_layout

\begin_layout Plain Layout

  Position observe()
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset

where the struct is the data structure, and observe is the cross-language
 function call.
\end_layout

\begin_layout Standard
Communication between vision and monkey module (http server)
\end_layout

\begin_layout Standard
Communication between monkey and simulation module
\end_layout

\begin_layout Subsection
Testing tools
\end_layout

\begin_layout Paragraph
GUI for human control
\end_layout

\begin_layout Standard
One of the first and most basic of the testing tools that was developed
 was a GUI to let the user control the helicopter manually from the laptop.
 This served as a suitable test for the capabilities of the Arduino communicatio
n between the computer and the helicopter.
 Additionally, it enabled the system to output the specific values that
 were being sent to the helicopter, something that was not possible from
 flying the helicopter using the remote control.
 As detailed below, it also played a part in some other parts of the testing
 tools.
\end_layout

\begin_layout Paragraph
DuoCopter
\end_layout

\begin_layout Standard
After experimenting with some basic control methods for the helicopter early
 on in the project, it became obvious that it would be useful to restrict
 (and thus simplify) the parts of the helicopter that the controller can
 actually control.
 However, it would then be necessary for something else to control the rest
 of the helicopter, otherwise it might fly out of control whilst testing.
 To fulfill this purpose, a special 
\begin_inset Quotes eld
\end_inset

Duocopter
\begin_inset Quotes erd
\end_inset

 flyer was created.
 This Duocopter created two copies of the same helicopter control, and gave
 one to each controller.
 Incoming instructions from both of the controllers would be received by
 the Duocopter, filtered according order type, and then sent onwards to
 the helicopter itself.
 This process is invisible to both of the controllers and the helicopter.
\begin_inset Newline newline
\end_inset

For example, initially it was useful to let the Monkey control the Throttle
 of the helicopter, whilst stabilising and steering were controlled by a
 human operator through the GUI.
 The Duocopter would throw away any instruction received from the Monkey
 that was not a Throttle instruction, and likewise for non-Throttle instructions
 from the human.
 In this manner it was possible to test the Monkey Throttle control in a
 simplified environment without having to worry about the helicopter crashing!
\end_layout

\begin_layout Paragraph
Viewing methods
\end_layout

\begin_layout Standard
In addition with testing tools for controlling the helicopter, it was deemed
 useful to be able to 
\begin_inset Quotes eld
\end_inset

see what the controller sees
\begin_inset Quotes erd
\end_inset

, that is to have a method of visualising the data that is received by the
 Monkey when it tries to observe the helicopter.
 Original concepts for this revolved around outputting an image of the current
 observed position of the helicopter every iteration, and then making an
 animation out of the produced images.
 However, this lacked the real-time aspect that is required with most testing
 tools.
 These images could instead be displayed in a window created by the Monkey
 - the downside to this is that it is not overwhelmingly easy to do in Haskell,
 which reduces the ability to iterate on the design.
\begin_inset Newline newline
\end_inset

The idea chosen in the end was to develop it in Python - Python has many
 pre-installed modules that deal exactly with window output in a very simple
 and concise manner.
 Using the Pygame
\begin_inset CommandInset citation
LatexCommand cite
key "Pygame"

\end_inset

 module (which is already used in the SimpleCV module used for vision),
 creating a window of a specific size was as simple as 
\begin_inset listings
lstparams "language=Python,numbers=left,tabsize=2"
inline false
status open

\begin_layout Plain Layout

def main():
\end_layout

\begin_layout Plain Layout

  pygame.init()
\end_layout

\begin_layout Plain Layout

  window = pygame.display.set_mode((640,480))
\end_layout

\end_inset

From here, a simple two dimensional viewer was developed, one for a 
\begin_inset Quotes eld
\end_inset

side-on
\begin_inset Quotes erd
\end_inset

 perspective, showing 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 data, and another 
\begin_inset Quotes eld
\end_inset

top-down
\begin_inset Quotes erd
\end_inset

 perspective showing 
\begin_inset Formula $x,y,z$
\end_inset

 and 
\begin_inset Formula $orientation$
\end_inset

 data.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename presentTop.png
	width 15cm

\end_inset


\end_layout

\begin_layout Standard
It was desirable to make the connection between the Monkey (in Haskell)
 and the Python viewer as simple as possible, to reduce any potential mistakes.
 The resulting method was simply using 
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

stdin/stdout
\end_layout

\end_inset

 - the Monkey would print the positional information, and then by piping
 the output of one to the other in a Bash shell, the viewer would read the
 output, parse it and then display it.
 
\begin_inset Newline newline
\end_inset

This proved to be a simple yet effective manner for co-ordinating the two
 programs, with no need to worry about complicated cross-language tools
 or synchronisation problems, as all of the threading is handled by the
 Bash shell itself.
\end_layout

\begin_layout Paragraph
Simulation
\end_layout

\begin_layout Itemize
VirtualHelicopter
\end_layout

\begin_layout Itemize
Control variations
\end_layout

\begin_layout Itemize
Accuracy of the simulation
\end_layout

\begin_layout Section
Vision
\end_layout

\begin_layout Standard
The vision system is an integral part to the project - it is needed to track
 the helicopter in its surroundings, and then report the 3D position of
 the helicopter (and orientation) back to the controller.
 As discussed earlier, this has to be achieved with one stationary Kinect
 (RGB-D) camera, and a laptop.
 Additionally, the tracking method has to have sufficient granularity so
 that the controller can get updates in real time.
 This section will cover the various steps taken to achieve this, pitfalls
 that were encountered, and how the current system works.
\end_layout

\begin_layout Subsection
Initial requirements and decisions
\end_layout

\begin_layout Standard
At the beginning of the project, the following requirements were assembled
 that were deemed to be necessary for 'basic flight':
\end_layout

\begin_layout Enumerate
To be able to get a reasonable estimate for the 3D coordinates of the helicopter
 in a stationary environment.
\end_layout

\begin_layout Enumerate
To be able to estimate the orientation of the helicopter from some prior
 knowledge and the 'tracked' image of the helicopter.
\end_layout

\begin_layout Enumerate
To be able to do the above 2 things sufficiently quickly that the system
 can be used in a real-time situation.
\end_layout

\begin_layout Enumerate
The system should be sufficiently robust that 'unavoidable' interference
 (such as subtle lighting changes, shadows etc.) in the scene should not
 impair the tracking to any great degree.
\end_layout

\begin_layout Standard
With these requirements in mind, initial research was carried out into possible
 languages to implement the vision system in, and what libraries to use.
 
\begin_inset Newline newline
\end_inset

The first consideration was to use Haskell, as the main control system is
 written entirely in Haskell.
 This would make integration very simple.
 However, after further research it was discovered that vision libraries
 for Haskell are slim, and not particularly well supported.
 Most of the libraries are cross-language, partial implementations of the
 C++ OpenCV libraries.
 Due to their partial and experimental nature, and previous inexperience
 with OpenCV, it was deemed too risky to attempt to proceed with the vision
 module in Haskell.
\begin_inset Newline newline
\end_inset

The second consideration was using C++ and OpenCV.
 This was an obvious consideration, as huge amounts of computer vision research
 is done in OpenCV and C++, with the language/library pair being considered
 the 'main' computer vision method.
 However, this approach was not entirely without flaw - inexperience with
 both C++ and vision in general made this quite daunting.
 Whilst a good choice if no other alternative are found, it seemed that
 a further search for a simpler language/implementation might prove to be
 beneficial.
\begin_inset Newline newline
\end_inset

The last consideration was the SimpleCV library, which is written in Python.
 The SimpleCV uses the Python implementation of OpenCV, but provides a large
 amount of 'simple' methods and data structures that hide the more complex
 data structures below.
 Additionally, it is well supported with many tutorials and examples.
 This seemed to fulfill the most requirements without many downsides, so
 development began using that.
\end_layout

\begin_layout Subsection
Basic Software and Setup
\end_layout

\begin_layout Standard
Before prototyping could begin, it was necessary to find pre-existing software
 packages that would perform a lot of the 'heavy lifting' of hardware management
 with the Kinect.
 Fortunately, all of this was well described by the SimpleCV documentation
 - for hardware drivers, the open source Kinect library OpenKinect (via
 freenect) 
\begin_inset CommandInset citation
LatexCommand cite
key "freenect website"

\end_inset

 was used.
 This provided basic Kinect functions for Python.
 The SimpleCV library is then built on top of this, adding extra functions
 for general image processing and video processing.
 Finally, it was necessary to calibrate the Kinect, which was also simple
 given the provision of a tutorial for calibration in the freenect package.
\end_layout

\begin_layout Subsection
RBG Vision
\end_layout

\begin_layout Standard
The first method that was tested for performing the position detection was
 based around using the RGB camera for initial detection, and using the
 depth camera to 'augment' the detection for accuracy and depth measurements.
 The founding for this idea was that the RGB camera was less noisy than
 the depth camera, which was prone to casting depth 'shadows', areas where
 no depth information could be recovered.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename shadow_example.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

Figure showing the appearance of 'depth-shadows', visualised as white areas
 in the image.
 The Kinect device is incapable of obtaining depth information these areas.
 These shadows are a by-product of the depth-detection method, and unfortunately
 cannot be avoided using the Kinect 'as-is'.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Groundwork
\end_layout

\begin_layout Standard
Following the assumptions stated in 5.3.1, an idea to initially locate the
 helicopter was to take one image of the static scene without the helicopter
 present, and then to subtract this base image from the new images that
 were being taken.
 As the helicopter was assumed to be the only moving object in the scene,
 this should in theory leave just the shape of the helicopter, which could
 be refined using standard imaging techniques.
 Having successfully segmented the helicopter from the rest of the scene,
 it was now necessary to work out the corresponding location of the helicopter
 in the depth map.
 This, however, was where the problems began.
 Without the depth information of the helicopter, it was impossible (or
 at least, very difficult) to reconstruct the position of the helicopter
 into three-dimensions - and without this 3D projection, it was not possible
 to project it again into the depth camera simply by using the RGB camera
 alone.
 
\begin_inset Newline newline
\end_inset

Several methods were tried to remedy this:
\end_layout

\begin_layout Description
Best-guess
\begin_inset space ~
\end_inset

search One method that was tried was locating the position of the helicopter
 in the RGB camera, getting shape information of the helicopter, and then
 trying to again locate this shape (or similar shapes) in an area that would
 approximately match that on the depth camera.
 Drawbacks of this method included additional overhead of doing image analysis
 twice, difficulty locating similar shapes in both cameras (as each camera
 had a different 'type' of error that it would pick up) and being somewhat
 overcomplicated.
\end_layout

\begin_layout Description
Estimating
\begin_inset space ~
\end_inset

camera
\begin_inset space ~
\end_inset

parameters Another naive method used was just to 'guess' where the helicopter
 would be in the depth camera from ad-hoc observations about relative positions
 in the depth and RGB images.
 This proved to not be accurate enough at all, and upon later reflection
 to be a bit of a waste of time.
 
\end_layout

\begin_layout Standard
It seemed that short of using the depth camera purely to locate the helicopter,
 it would be very difficult to recreate the helicopter in three dimensions
 using the RGB camera.
 So that is exactly what we tried to do next (Section 5.3.4).
\end_layout

\begin_layout Subsubsection
Prototyping results
\end_layout

\begin_layout Itemize
Stable lighting problems
\end_layout

\begin_layout Itemize
Shadows
\end_layout

\begin_layout Itemize
Similar colours
\end_layout

\begin_layout Subsection
D Vision
\end_layout

\begin_layout Standard
Having tested out the RGB camera methods, the next idea was to try to recreate
 the best parts of the RGB detection using just the depth camera, whilst
 trying to minimise the downsides of the depth camera itself (as mentioned
 earlier, one of the main problems was excessive noise in depth images caused
 by 'depth-shadowing').
 
\end_layout

\begin_layout Subsubsection
Groundwork
\end_layout

\begin_layout Standard
There were a few differences between the depth camera and the RGB camera
 that needed to be considered before development could be started on the
 depth detection.
\begin_inset Formula 
\[
\begin{array}{ccc}
 & \mbox{RGB} & \mbox{D}\\
Noise & \mbox{Not noisy} & \mbox{Noisy}\\
Dimensions & (0-255)^{3} & (0-2047)\\
Errors & \mbox{No} & \mbox{Yes}\\
Spectrum & \mbox{Visible light} & \mbox{IR light}
\end{array}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Noise
\end_layout

\begin_layout Standard
One of the main benefits of the RGB images was that they were not overly
 prone to noise - as long as the objects in the scene remained stationary,
 two consecutive images would be essentially identical.
 The depth images, by comparison, were very prone to noise as the depth-shadows
 moved in the scene.
 This meant that an otherwise stationary scene would still cause noise in
 a naive subtraction detection algorithm.
\end_layout

\begin_layout Paragraph
Dimensions
\end_layout

\begin_layout Standard
The RGB camera operates with 3 dimensions of values (as the name suggests)
 - 0-255 for each of Red, Green and Blue.
 This means that there is a total of 16581375 different values for each
 pixel.
 The depth camera, by contrast, only takes values from 0-2047, with 2047
 being the error value.
 Whilst this may seem that the detail provided is significantly less, in
 practise this does not affect the ability of the camera to detect the helicopte
r to any noticeable degree.
\end_layout

\begin_layout Paragraph
Errors
\end_layout

\begin_layout Standard
Every pixel on an RGB image was valid - the closest that one could get to
 an error value on an RGB image was a 'too light' or 'too dark' area, which
 would manifest themselves as the values 0 or 255 respectively.
 For the depth images, this was also true - too close values would appear
 as 0, and too far values would appear as 2046, however in addition areas
 that could not be properly reconstructed would appear as 2047, the error
 value.
\end_layout

\begin_layout Paragraph
Spectrum
\end_layout

\begin_layout Standard
This is a fairly obvious difference - the RGB camera operates purely over
 visible light, whereas the depth camera operated using IR light.
 This was the key factor that set the depth camera apart from the RGB camera
 - it meant that it was not sensitive to changing lighting conditions, or
 to shadows cast by objects or the helicopter.
\end_layout

\begin_layout Subsubsection
Prototyping results
\end_layout

\begin_layout Subsection
Orientation Detection
\end_layout

\begin_layout Standard
The second vital part of the helicopter detection is finding out which way
 the helicopter is facing - without this knowledge, it is impossible to
 steer the helicopter in any particular direction.
 Orientation detection presented an interesting challenge, and was probably
 the more complicated part of the Vision module.
 This complexity was exaggerated by the low resolution of the RGBD images
 - both of which operate at 640x480 pixels.
 At any one point, it is unlikely that the helicopter would take up more
 than a 100x100 pixel square in the original image!
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename r_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
An example image of the helicopter after basic position detection.
 Basic detail is obtained, but the distinction between helicopter and background
 is less obvious on the metal chassis and on the blades themselves.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Groundwork
\end_layout

\begin_layout Standard
From the previous work with helicopter detection, it was obvious that using
 the depth camera to also perform orientation analysis would not be sufficient.
 The reflective property of the helicopter, combined with its small size
 and the low fidelity of the depth image meant that a meaningful capture
 of the helicopter would be too difficult.
 Instead, we decided to focus on using the RGB camera.
 Figure 5.2 appears to show that we can get enough information from a well-lit
 RGB image to discern the orientation.
\end_layout

\begin_layout Paragraph
Naive extraction
\end_layout

\begin_layout Standard
As an initial stepping stone, a basic extraction method using image subtraction
 was used.
 This was a two-step process - given a 'base' image of the scene, without
 the helicopter, and another with the helicopter in it:
\end_layout

\begin_layout Enumerate
Take the absolute difference between the two images to obtain a greyscale
 'difference' image.
\end_layout

\begin_layout Enumerate
Perform image thresholding on the resulting image, keeping values that are
 different enough and discarding the rest (being regarded as image noise).
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python,tabsize=2"
inline false
status open

\begin_layout Plain Layout

# Get the numpy array of values
\end_layout

\begin_layout Plain Layout

n1 = base.getNumpy() 
\end_layout

\begin_layout Plain Layout

n2 = img.getNumpy()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Take the absolute difference between them
\end_layout

\begin_layout Plain Layout

diff = Image(absdiff(n1,n2))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Perform basic thresholding and erosion to get 
\end_layout

\begin_layout Plain Layout

# rid of noise and leave a black and white mask
\end_layout

\begin_layout Plain Layout

mask = diff.binarize().erode()
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This gave a reasonably good depiction of the outline of the helicopter as
 can be seen in Figure 5.3.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename h_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

Extraction of the helicopter from the image in Figure 5.2, using the subtractive
 method.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Signal decomposition
\end_layout

\begin_layout Standard
One suggested method of orientation analysis was to decompose the silhouette
 of the helicopter into height along the y-axis.
 The basis of this was that the front of the helicopter has more 'stuff'
 to it - it is taller and rounder than the tail of the helicopter, which
 is comparitively small.
 Thus the decomposition would result in a graph that is taller at one end
 and shorter at the other, and the distance between these two features would
 discern the orientation of the helicopter.
\begin_inset Newline newline
\end_inset

Initial tests with this method were not particularly conclusive - a clear
 peak can be 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename g_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

Initial horizontal signal decomposition of the helicopter.
 A peak is clearly present on the left hand side, comprised of the two helicopte
r blades and the main body of the helicopter.
 
\end_layout

\end_inset

seen for the two helicopter blades and the main body of the helicopter.
 However, it is unclear where the body of the helicopter ends - this is
 caused by the fact that the helicopter blades take up a large portion of
 the length of the helicopter.
 Another problem that they cause is they are quite noisy - their detected
 position in any subsequent images may be different and thus subsequently
 generated graphs would also be quite noisy.
\begin_inset Newline newline
\end_inset

To attempt to remedy this, a simple method was devised to remove the helicopter
 blades from the scene.
 The helicopter always has two large shadows, one for each helicopter blade.
 By looking at the horizontal scanlines, rather than the vertical scanlines
 (Figure 5.5) it is possible to see that these are clearly identifiable as
 the first two 'large' peaks.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename gv_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

Vertical decomposition of the helicopter signal.
 The two large peaks on the left are caused by the two main blades of the
 helicopter.
\end_layout

\end_inset

By cropping the portion of the image that holds these two peaks, (achived
 by searching linearly from the left until two 'large' peaks are met.
 The exact definition of large is relative to the rest of the signal) it
 is possible to remove the blades from the image entirely, providing the
 clearer horizontal decomposition in Figure 5.6.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename gh_000.jpg
	width 10cm

\end_inset


\end_layout

\end_inset

New horizontal decomposition, with helicopter blades removed.
 Now two entities are clearly visible - the large cockpit of the helicopter
 on the left, and the smaller tail rotor on the right.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Sift descriptor
\end_layout

\begin_layout Standard
After discussing the above findings with the supervisor, it was decided
 that it might be beneficial to try an alternate approach, using a 'Sift
 Descriptor'.
\end_layout

\begin_layout Subsection
Orientation Inferrence
\end_layout

\begin_layout Standard
Another, simpler, method of orientation analysis attempted was inferrence.
 This worked on the underlying assumption that as the helicopter is incapable
 of 'strafing' motion (that is motion to the left or right whilst facing
 forwards), provided there is some forwards pitch the helicopter will move
 in the direction it is facing.
 Taking an average of the 
\begin_inset Formula $N$
\end_inset

 most recent velocities from the 
\begin_inset Formula $N+1$
\end_inset

 positions recorded dampens most of the observational noise, and provides
 a fairly accurate representation of the current heading - larger values
 of 
\begin_inset Formula $N$
\end_inset

 will be slower to react to velocity changes, yet will be more resilient
 to noise.
 These velocities are averaged, and the heading is then extracted (in our
 case, values 0 - 359 are used, with 0 being directly away from the camera,
 increasing clockwise).
\end_layout

\begin_layout Subsubsection
Rough results
\end_layout

\begin_layout Standard
Given a suitable constant forward velocity, the orientation inferrence method
 gave positive results for a simple steering mechanism.
 It suffered somewhat when the helicopter was not operating under ideal
 conditions (such as backdraft from walls causing the helicopter to move
 backwards), however given the simplicity of the system and the hoped operating
 conditions of the helicopter, it still proved to be an attractive method.
\end_layout

\begin_layout Section
Control
\end_layout

\begin_layout Standard
Accurate detection was only one of the requirements that needed to be fulfilled
 - armed with positi4onal data, it was now important to decide how to control
 the helicopter itself.
 Fortunately, the design of the helicopter itself allows for conceptual
 simple controls for a human controller, and as such it is (relatively)
 simple to automate also.
 
\begin_inset Newline newline
\end_inset

The control of the helicopter can be split into two separate areas with
 minimal overlap.
 The first of these is the vertical motion of the helicopter, governed by
 the main rotors and the throttle - increasing the throttle of the helicopter
 results in an increase in vertical velocity and vice versa for decreasing
 throttle.
 The second area is motion in the 
\begin_inset Formula $x-z$
\end_inset

 plane, which is controlled by the pitch and yaw of the helicopter.
 Due to the simplistic design, the helicopter is incapable of deliberate
 strafing motion left and right (of course it is still susceptible to sideways
 drift from air currents or its own momentum), so navigation is accomplished
 by pointing the helicopter in the desired direction, then changing the
 pitch until the desired velocity is acquired.
 These two areas are mostly disjoint, however there is still a small amount
 of overlap.
 The degree of forward motion provided by a change of pitch is increased
 accordingly with the amount of throttle currently being used, and corresponding
ly the upwards velocity is decreased as the pitch increases (as upwards
 velocity becomes a diagonal-forward velocity.
 The decrease is proportional to 
\begin_inset Formula $\cos\theta$
\end_inset

, where 
\begin_inset Formula $\theta$
\end_inset

 is the angle between the 
\begin_inset Formula $y-axis$
\end_inset

 and the upward vector of the helicopter).
 
\begin_inset Newline newline
\end_inset

Based on these observations, it is possible to construct several different
 control methods.
\end_layout

\begin_layout Subsection
Control methods
\end_layout

\begin_layout Itemize
How helicopter will be controlled
\end_layout

\begin_layout Itemize
Started considering how a human does it
\end_layout

\begin_layout Itemize
PID
\end_layout

\begin_layout Paragraph
Intentions
\end_layout

\begin_layout Paragraph
Enacting Intentions
\end_layout

\begin_layout Itemize
Simple point and drive
\end_layout

\begin_layout Itemize
Combinations
\end_layout

\begin_layout Section
Additional Features
\end_layout

\begin_layout Subsection
Landing and Taking Off
\end_layout

\begin_layout Subsection
Assisted flying
\end_layout

\begin_layout Chapter
Evaluation
\end_layout

\begin_layout Paragraph
Required functionality
\end_layout

\begin_layout Enumerate
Automated flight with no input other than camera.
\end_layout

\begin_layout Enumerate
Input of 'intentions', and automatic execution of them.
\end_layout

\begin_layout Enumerate
Movement in one, two and three dimensions.
\end_layout

\begin_layout Standard
I believe that these three points establish the 'baseline' of the functionality
 I should achieve.
 Successful execution of these should demonstrate complexity in computer
 vision, AI and general design, and should be sufficiently challenging to
 perform.
 
\end_layout

\begin_layout Section
Quality of functionality
\end_layout

\begin_layout Standard
It should prove to be quite simple to ascertain the quality of the functionality
:
\end_layout

\begin_layout Description
Observation It will be fairly evident from how the helicopter flies according
 to the given instructions how well the project is performing.
 For instance, a steady, certain flight path would be good whereas a juddering
 path, or even crashing, would indicate bad performance!
\end_layout

\begin_layout Description
Execution The ability to execute more and more complicated intentions.
 Simply hovering in a given position should prove to be a (comparatively)
 simple feat, whereas flying from one position to another is more demanding.
 Extending this to customisable paths in three dimensional space would show
 that the underlying mechanisms are of high quality and performance.
\end_layout

\begin_layout Description
Extensibility If the system is set up well, it should be easy to extend
 it with new abilities and features, using the existing functionality.
\end_layout

\begin_layout Standard
These could be measured using the following demonstration, for example:
\end_layout

\begin_layout Enumerate
Demonstrate the ability to fly the helicopter from the computer manually.
 
\shape italic
Demonstrates the foundations of the project.
 
\end_layout

\begin_layout Enumerate
Demonstrate the ability to hover the helicopter in place, even when displaced.
 
\shape italic
Demonstrates the static control of the helicopter from a single camera source
 in multiple dimensions.
 
\end_layout

\begin_layout Enumerate
Demonstrate the ability to move in a predefined path.
 
\shape italic
Demonstrates advanced functionality.
 
\end_layout

\begin_layout Section
Qualitative analysis of the system
\end_layout

\begin_layout Standard
To evaluate the limitations of the system, several tests were set up.
 These tests were designed to quantitavely see the exact capabilities and
 limiations of the system.
 Qualities tested (split into traits) were:
\end_layout

\begin_layout Paragraph
Position detection (vision)
\end_layout

\begin_layout Itemize
Range of detection
\end_layout

\begin_layout Itemize
Robustness of detection
\end_layout

\begin_layout Itemize
Accuracy of detection
\end_layout

\begin_layout Itemize
Limitations
\end_layout

\begin_layout Paragraph
Orientation detection (vision)
\end_layout

\begin_layout Itemize
Accuracy of detection
\end_layout

\begin_layout Itemize
Limitations
\end_layout

\begin_layout Paragraph
System properties
\end_layout

\begin_layout Itemize
Helicopter response times
\end_layout

\begin_layout Itemize
Multiple signals
\end_layout

\begin_layout Itemize
Control range/robustness (Arduino)
\end_layout

\begin_layout Itemize
Problems with the system
\end_layout

\begin_layout Paragraph
Helicopter properties
\end_layout

\begin_layout Itemize
Speeds of helicopter
\end_layout

\begin_layout Itemize
Flight time of helicopter
\end_layout

\begin_layout Itemize
Range of motion of helicopter
\end_layout

\begin_layout Section
Achievements
\end_layout

\begin_layout Section
Strengths and weaknesses of the system
\end_layout

\begin_layout Section
Comparison to existing technology
\end_layout

\begin_layout Chapter
Conclusions
\end_layout

\begin_layout Section
Learning results of this project
\end_layout

\begin_layout Section
Future work
\end_layout

\begin_layout Paragraph*
Multiple Helicopters
\end_layout

\begin_layout Paragraph
Use in Moving Environments
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Syma"

\end_inset

Lee, J.
 (2011) Procrastineering - Project blog for Johnny Chung Lee: Computer Controlli
ng a Syma Helicopter.
 [online] Available at: http://procrastineering.blogspot.co.uk/2011/11/computer-con
trolling-syma-helicopter.html [Accessed: 1 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Syma2"

\end_inset

Avergottini.com (2011) Couch Sprout: Arduino helicopter infrared controller.
 [online] Available at: http://www.avergottini.com/2011/05/arduino-helicopter-infr
ared-controller.html [Accessed: 1 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Arduino"

\end_inset

Arduino.cc (n.d.) Arduino - Guide.
 [online] Available at: http://arduino.cc/en/Guide/Guide [Accessed: 1 Jan
 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "IR"

\end_inset

Ladyada.net (2012) Sensor tutorials - IR remote receiver/decoder tutorial.
 [online] Available at: http://www.ladyada.net/learn/sensors/ir.html [Accessed:
 1 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Arduino port"

\end_inset

Arduino.cc (n.d.) Arduino - PortManipulation.
 [online] Available at: http://www.arduino.cc/en/Reference/PortManipulation
 [Accessed: 1 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "freenect paper"

\end_inset

Conley, K.
 (2012) Adding Video Support to Christopher Doneâs Haskell Kinect Library.
 [e-book] http://static.kevintechnology.com/docs/cis194.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "freenect"

\end_inset

kevincon (2012) freenect.
 [online] Available at: https://github.com/kevincon/freenect [Accessed: 4
 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Least squares + Recursive"

\end_inset

Unknown.
 (2010) Recursive Least Squares Estimation.
 [e-book] Available through: http://www.cs.iastate.edu http://www.cs.iastate.edu/~cs57
7/handouts/recursive-least-squares.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Recursive least squares"

\end_inset

Unknown.
 (n.d.) Lecture 10: Recursive Least Squares Estimation.
 [e-book] Available through: http://www.cs.tut.fi http://www.cs.tut.fi/~tabus/course/A
SP/LectureNew10.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Least squares wiki"

\end_inset

En.wikipedia.org (2012) Least squares - Wikipedia, the free encyclopedia.
 [online] Available at: http://en.wikipedia.org/wiki/Least_squares [Accessed:
 4 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "OpenCV book"

\end_inset

BRADSKI, G.
 R., & KAEHLER, A.
 (2008).
 Learning OpenCV: computer vision with the OpenCV library.
 Sebastopol, CA, O'Reilly.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "OpenCV examples"

\end_inset

aleator (2012) CV.
 [online] Available at: https://github.com/aleator/CV/tree/master/examples
 [Accessed: 5 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "SLAM"

\end_inset

En.wikipedia.org (2005) Simultaneous localization and mapping - Wikipedia,
 the free encyclopedia.
 [online] Available at: http://en.wikipedia.org/wiki/Simultaneous_localization_and
_mapping [Accessed: 7 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Camera based nav of low cost quadropter"

\end_inset

J.
 Engel, J.
 Sturm, D.
 Cremers (2012) Camera-Based Navigation of a Low-Cost Quadrocopter .
 [e-book] Available through: http://vision.in.tum.de/research/quadcopter http://vis
ion.in.tum.de/_media/spezial/bib/engel12iros.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Autonomous Camera based quadropter"

\end_inset

Engel, J.
 (2011) Autonomous Camera-Based Navigation of a Quadrocopter .
 Masters.
 Der Technischen Universitat Munchen.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Kinectfusion"

\end_inset

Izadi, S.
 et al.
 (n.d.) KinectFusion: Real-time 3D Reconstruction and Interaction Using a
 Moving Depth Camera*.
 [e-book] Available through: http://research.microsoft.com/ http://research.microso
ft.com/pubs/155416/kinectfusion-uist-comp.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Odometry quadropter"

\end_inset

Kerl, C.
 (2012) Odometry from RGB-D Cameras for Autonomous Quadrocopters .
 [e-book] Available through: http://vision.in.tum.de/research/quadcopter http://vis
ion.in.tum.de/_media/spezial/bib/kerl2012msc.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Kalman wiki"

\end_inset

En.wikipedia.org (1960) Kalman filter - Wikipedia, the free encyclopedia.
 [online] Available at: http://en.wikipedia.org/wiki/Kalman_filter [Accessed:
 16 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "kalman tutorial"

\end_inset

Cs.unc.edu (2012) The Kalman Filter.
 [online] Available at: http://www.cs.unc.edu/~welch/kalman/ [Accessed: 16
 Jan 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "SimpleCV book"

\end_inset

DEMAAGD, K., OLIVER, A., & OOSTENDORP, N.
 (2012).
 Practical computer vision with SimpleCV.
 Beijing, O'Reilly.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Pygame"

\end_inset

Pygame.org (2012) News - pygame - python game development.
 [online] Available at: http://www.pygame.org [Accessed: 25 Apr 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "PID Wiki"

\end_inset

En.wikipedia.org (1890) PID controller - Wikipedia, the free encyclopedia.
 [online] Available at: http://en.wikipedia.org/wiki/PID_controller [Accessed:
 20 May 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "PID Tuning"

\end_inset

Zhong, J.
 (2006) PID Controller Tuning: A short tutorial.
 [e-book] http://wwwdsa.uqac.ca/~rbeguena/Systemes_Asservis/PID.pdf.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "freenect website"

\end_inset

Openkinect.org (2000) OpenKinect.
 [online] Available at: http://openkinect.org/wiki/Main_Page [Accessed: 21
 May 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "FFI"

\end_inset

Haskell.org (n.d.) 8 Foreign Function Interface.
 [online] Available at: http://www.haskell.org/onlinereport/haskell2010/haskellch8.
html [Accessed: 29 May 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Thrift"

\end_inset

Thrift.apache.org (2012) Apache Thrift.
 [online] Available at: http://thrift.apache.org/ [Accessed: 29 May 2013].
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "teensyusb"

\end_inset

http://www.pjrc.com/teensy/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ludic"

\end_inset

Colton, S.
 and Pease, A.
 2011.
 Lecture 9: Steering Behaviours.
 [e-book] pp.15 - 16.
 Available through: Ludic Computing, Department of Computing [Accessed:
 09/06/2013].
\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Appendix
\end_layout

\begin_layout Section*
User Guide
\end_layout

\begin_layout Itemize
How to use the helicopter flight system
\end_layout

\end_body
\end_document
